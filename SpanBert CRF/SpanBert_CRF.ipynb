{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc6c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm, trange\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d2e4c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "cuda_yes = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:1\" if cuda_yes else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09849a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346b56a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for NER.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "          guid: Unique id for the example(a sentence or a pair of sentences).\n",
    "          words: list of words of sentence\n",
    "          labels_a/labels_b: (Optional) string. The label seqence of the text_a/text_b. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        # list of words of the sentence,example: [EU, rejects, German, call, to, boycott, British, lamb .]\n",
    "        self.words = words\n",
    "        # list of label sequence of the sentence,like: [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]\n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e47e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\n",
    "    result of convert_examples_to_features(InputExample)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids,  predict_mask, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.predict_mask = predict_mask\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55c7351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CoNLLDataProcessor():\n",
    "    '''\n",
    "    CoNLL-2003\n",
    "    '''\n",
    "\n",
    "    def __init__(self, out_lists):\n",
    "        self.data = out_lists\n",
    "#         shuffle(self.data)\n",
    "        self._label_types = ['B-Command', 'B-Error', 'B-Extension', 'B-Software_Component', 'B-Peripheral',\n",
    "       'B-OS', 'B-Package', 'B-Architecture', 'B-Organization', 'I-Command', 'I-Error', 'I-Extension',\n",
    "       'I-Software_Component', 'I-Peripheral', 'I-OS', 'I-Package', 'I-Architecture', 'I-Organization',\n",
    "                             '[CLS]' , '[SEP]' , 'O']\n",
    "        self._num_labels = len(self._label_types)\n",
    "        self._label_map = {label: i for i,\n",
    "                           label in enumerate(self._label_types)}\n",
    "        self.train_data, self.test_data = train_test_split(self.data, test_size=0.2, random_state=1)\n",
    "        self.train_data, self.valid_data = train_test_split(self.train_data, test_size=0.25, random_state=1)\n",
    "\n",
    "    def get_train_examples(self):\n",
    "#         return self._create_examples(self.data[:len(self.data) * 3 // 5 ])\n",
    "        return self._create_examples(self.train_data)\n",
    "    \n",
    "    def get_valid_examples(self):\n",
    "#         return self._create_examples(self.data[len(self.data) * 3 // 5:len(self.data) * 4 // 5])\n",
    "        return self._create_examples(self.valid_data)\n",
    "    \n",
    "    def get_test_examples(self):\n",
    "#         return self._create_examples(self.data[len(self.data) * 4 // 5:])\n",
    "        return self._create_examples(self.test_data)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self._label_types\n",
    "\n",
    "    def get_num_labels(self):\n",
    "        return self._num_labels\n",
    "    \n",
    "    def get_label_map(self):\n",
    "        return self._label_map\n",
    "    \n",
    "    def get_start_label_id(self):\n",
    "        return self._label_map['[CLS]']\n",
    "\n",
    "    def get_stop_label_id(self):\n",
    "        return self._label_map['[SEP]']\n",
    "\n",
    "    def _create_examples(self, all_lists):\n",
    "        examples = []\n",
    "        for (i, one_lists) in enumerate(all_lists):\n",
    "            guid = i\n",
    "            words = one_lists[0]\n",
    "            labels = one_lists[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, words=words, labels=labels))\n",
    "        return examples\n",
    "\n",
    "    def _create_examples2(self, lines):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = i\n",
    "            text = line[0]\n",
    "            ner_label = line[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, text_a=text, labels_a=ner_label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b7a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def example2feature(example, tokenizer, label_map, max_seq_length):\n",
    "\n",
    "    add_label = 'X'\n",
    "    # tokenize_count = []\n",
    "    tokens = ['[CLS]']\n",
    "    predict_mask = [0]\n",
    "    label_ids = [label_map['[CLS]']]\n",
    "    for i, w in enumerate(example.words):\n",
    "        # use bertTokenizer to split words\n",
    "        # 1996-08-22 => 1996 - 08 - 22\n",
    "        # sheepmeat => sheep ##me ##at\n",
    "        try:\n",
    "            if(pd.isna(w)):\n",
    "                sub_words = ['[UNK]']\n",
    "            else:\n",
    "                sub_words = tokenizer.tokenize(w)\n",
    "        except:\n",
    "            print(pd.isna(w))\n",
    "            print(type(w))\n",
    "            sub_words = tokenizer.tokenize(w)\n",
    "        if not sub_words:\n",
    "            sub_words = ['[UNK]']\n",
    "        # tokenize_count.append(len(sub_words))\n",
    "        tokens.extend(sub_words)\n",
    "        for j in range(len(sub_words)):\n",
    "            if j == 0:\n",
    "                predict_mask.append(1)\n",
    "                label_ids.append(label_map[example.labels[i]])\n",
    "            else:\n",
    "                # '##xxx' -> 'X' (see bert paper)\n",
    "                predict_mask.append(0)\n",
    "                label_ids.append(label_map[example.labels[i]])\n",
    "\n",
    "    # truncate\n",
    "    if len(tokens) > max_seq_length - 1:\n",
    "        #print('Example No.{} is too long, length is {}, truncated to {}!'.format(example.guid, len(tokens), max_seq_length))\n",
    "        tokens = tokens[0:(max_seq_length - 1)]\n",
    "        predict_mask = predict_mask[0:(max_seq_length - 1)]\n",
    "        label_ids = label_ids[0:(max_seq_length - 1)]\n",
    "    tokens.append('[SEP]')\n",
    "    predict_mask.append(0)\n",
    "    label_ids.append(label_map['[SEP]'])\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    feat=InputFeatures(\n",
    "                # guid=example.guid,\n",
    "                # tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                predict_mask=predict_mask,\n",
    "                label_ids=label_ids)\n",
    "\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f82639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(data.Dataset):\n",
    "    def __init__(self, examples, tokenizer, label_map, max_seq_length):\n",
    "        self.examples=examples\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label_map=label_map\n",
    "        self.max_seq_length=max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat=example2feature(self.examples[idx], self.tokenizer, \n",
    "                             self.label_map, self.max_seq_length)\n",
    "        return feat.input_ids, feat.input_mask, feat.segment_ids, feat.predict_mask, feat.label_ids\n",
    "\n",
    "    @classmethod\n",
    "    def pad(cls, batch):\n",
    "\n",
    "        seqlen_list = [len(sample[0]) for sample in batch]\n",
    "        maxlen = np.array(seqlen_list).max()\n",
    "\n",
    "        f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: X for padding\n",
    "        input_ids_list = torch.LongTensor(f(0, maxlen))\n",
    "        input_mask_list = torch.LongTensor(f(1, maxlen))\n",
    "        segment_ids_list = torch.LongTensor(f(2, maxlen))\n",
    "        predict_mask_list = torch.LongTensor(f(3, maxlen))\n",
    "        label_ids_list = torch.LongTensor(f(4, maxlen))\n",
    "\n",
    "        return input_ids_list, input_mask_list, segment_ids_list, predict_mask_list, label_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0e916b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bug Id</th>\n",
       "      <th>Start Index</th>\n",
       "      <th>End Index</th>\n",
       "      <th>Entity</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>BIO Tag</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>143211</td>\n",
       "      <td>67</td>\n",
       "      <td>72</td>\n",
       "      <td>which</td>\n",
       "      <td>WDT</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>143211</td>\n",
       "      <td>73</td>\n",
       "      <td>75</td>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>143211</td>\n",
       "      <td>76</td>\n",
       "      <td>82</td>\n",
       "      <td>called</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>143211</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>143211</td>\n",
       "      <td>86</td>\n",
       "      <td>89</td>\n",
       "      <td>dav</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>143211</td>\n",
       "      <td>89</td>\n",
       "      <td>91</td>\n",
       "      <td>'s</td>\n",
       "      <td>POS</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>143211</td>\n",
       "      <td>92</td>\n",
       "      <td>99</td>\n",
       "      <td>PROPGET</td>\n",
       "      <td>NNP</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>143211</td>\n",
       "      <td>100</td>\n",
       "      <td>102</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>143211</td>\n",
       "      <td>103</td>\n",
       "      <td>111</td>\n",
       "      <td>retrieve</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>143211</td>\n",
       "      <td>112</td>\n",
       "      <td>115</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Bug Id  Start Index  End Index    Entity POS Tag BIO Tag  year\n",
       "10  143211           67         72     which     WDT       O  2004\n",
       "11  143211           73         75        is     VBZ       O  2004\n",
       "12  143211           76         82    called     VBN       O  2004\n",
       "13  143211           83         85        on      IN       O  2004\n",
       "14  143211           86         89       dav      NN       O  2004\n",
       "15  143211           89         91        's     POS       O  2004\n",
       "16  143211           92         99   PROPGET     NNP       O  2004\n",
       "17  143211          100        102        to      TO       O  2004\n",
       "18  143211          103        111  retrieve      VB       O  2004\n",
       "19  143211          112        115       the      DT       O  2004"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_df = pd.read_csv('~/sr_drive/Avik/Annotations/results/updated_merged_BIO_tagging_removing_verb_adjacent_pos_tags.csv')\n",
    "data_df.iloc[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41bacdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_458584/1282516970.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  data_group = data_df.groupby(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bug Id</th>\n",
       "      <th>Entity</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>BIO Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>[Like, subscribing, to, a, bug, ,, I, 'd, like...</td>\n",
       "      <td>[IN, VBG, TO, DT, NN, ,, PRP, MD, VB, TO, VB, ...</td>\n",
       "      <td>[O, O, O, O, B-Package, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3165</td>\n",
       "      <td>[Symptoms, ========, Launchpad, sends, notific...</td>\n",
       "      <td>[NNS, VBP, NNP, VBZ, NNS, TO, NNS, IN, JJ, NNS...</td>\n",
       "      <td>[O, O, B-Organization, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3651</td>\n",
       "      <td>[I, 'm, looking, at, bzr, baz-import, ,, which...</td>\n",
       "      <td>[PRP, VBP, VBG, IN, JJ, NN, ,, WDT, VBZ, IN, P...</td>\n",
       "      <td>[O, O, O, O, B-Package, B-Command, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3666</td>\n",
       "      <td>['Track, Artist, ', is, a, great, concept, and...</td>\n",
       "      <td>[CD, NNP, '', VBZ, DT, JJ, NN, CC, PRP, VBP, P...</td>\n",
       "      <td>[O, B-Package, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3718</td>\n",
       "      <td>[While, installing, packages, ,, one, may, exp...</td>\n",
       "      <td>[IN, VBG, NNS, ,, CD, MD, VB, DT, NN, TO, VB, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-Package, O, O, O, B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bug Id                                             Entity  \\\n",
       "0     501  [Like, subscribing, to, a, bug, ,, I, 'd, like...   \n",
       "1    3165  [Symptoms, ========, Launchpad, sends, notific...   \n",
       "2    3651  [I, 'm, looking, at, bzr, baz-import, ,, which...   \n",
       "3    3666  ['Track, Artist, ', is, a, great, concept, and...   \n",
       "4    3718  [While, installing, packages, ,, one, may, exp...   \n",
       "\n",
       "                                             POS Tag  \\\n",
       "0  [IN, VBG, TO, DT, NN, ,, PRP, MD, VB, TO, VB, ...   \n",
       "1  [NNS, VBP, NNP, VBZ, NNS, TO, NNS, IN, JJ, NNS...   \n",
       "2  [PRP, VBP, VBG, IN, JJ, NN, ,, WDT, VBZ, IN, P...   \n",
       "3  [CD, NNP, '', VBZ, DT, JJ, NN, CC, PRP, VBP, P...   \n",
       "4  [IN, VBG, NNS, ,, CD, MD, VB, DT, NN, TO, VB, ...   \n",
       "\n",
       "                                             BIO Tag  \n",
       "0  [O, O, O, O, B-Package, O, O, O, O, O, O, O, O...  \n",
       "1  [O, O, B-Organization, O, O, O, O, O, O, O, O,...  \n",
       "2  [O, O, O, O, B-Package, B-Command, O, O, O, O,...  \n",
       "3  [O, B-Package, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "4  [O, O, O, O, O, O, O, O, B-Package, O, O, O, B...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_group = data_df.groupby(\n",
    "['Bug Id'],as_index=False\n",
    ")['Entity', 'POS Tag', 'BIO Tag'].agg(lambda x: list(x))\n",
    "\n",
    "data_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e44bb108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If I insert in my CD-RW drive a CD labelled \"1\", then click on the diskmounter\n",
      "applet, the 2 choices I get will be \"Open CD-RW(null) Drive\" and \"Eject\n",
      "CD-RW(null) Drive\".  In Hoary, the choices would be the nicer \"Open 1\" and\n",
      "\"Eject 1\".\n",
      "\n",
      "http://bugzilla.gnome.org/show_bug.cgi?id=310300: http://bugzilla.gnome.org/show_bug.cgi?id=310300\n"
     ]
    }
   ],
   "source": [
    "# folder_path = '/content/drive/MyDrive/BTP/bugdataset/'\n",
    "folder_path = '~/sr_drive/Avik/Annotations/'\n",
    "bug_description_path = folder_path\n",
    "\n",
    "bug_description_df = pd.read_csv(bug_description_path + 'descriptions_merged.csv')\n",
    "\n",
    "for _, row in bug_description_df.iterrows():\n",
    "    if(row['Bug Id'] == 18886):\n",
    "        print(row['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e23034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "out_lists = []\n",
    "flag = 0\n",
    "for idx, row in data_group.iterrows():\n",
    "#     for w_idx, w in enumerate(row['Entity']):\n",
    "#         if(pd.isna(w)):\n",
    "#             print(w_idx)\n",
    "#             print(row['Entity'])\n",
    "#             print(row['Bug Id'])\n",
    "#             print(row['BIO Tag'][w_idx])\n",
    "#             flag += 1\n",
    "#             break\n",
    "#     if flag == 2:\n",
    "#         break\n",
    "    out_lists.append([row['Entity'], row['BIO Tag']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4094797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDistribution(examples):\n",
    "    labels = []\n",
    "    for example in examples:\n",
    "        labels.extend(example.labels)\n",
    "    label_frequency = {}\n",
    "\n",
    "    # iterating over the list\n",
    "    for label in labels:\n",
    "       # checking the element in dictionary\n",
    "        if label in label_frequency:\n",
    "          # incrementing the counr\n",
    "            label_frequency[label] += 1\n",
    "        else:\n",
    "          # initializing the count\n",
    "            label_frequency[label] = 1\n",
    "\n",
    "    # printing the frequency\n",
    "    print(label_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de8264f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conllProcessor = CoNLLDataProcessor(out_lists)\n",
    "label_list = conllProcessor.get_labels()\n",
    "label_map = conllProcessor.get_label_map()\n",
    "train_examples = conllProcessor.get_train_examples()\n",
    "valid_examples = conllProcessor.get_valid_examples()\n",
    "test_examples = conllProcessor.get_test_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb7d36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('SpanBERT/spanbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acbef4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NerDataset(train_examples, tokenizer, label_map, 512)\n",
    "valid_dataset = NerDataset(valid_examples, tokenizer, label_map, 512)\n",
    "test_dataset = NerDataset(test_examples, tokenizer, label_map, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8501a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51e38b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=1,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "valid_dataloader = data.DataLoader(dataset=valid_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=1,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "test_dataloader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=1,\n",
    "                                collate_fn=NerDataset.pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e97209db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanBERT_CRF(nn.Module):\n",
    "    def __init__(self, num_labels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.spanbert = AutoModel.from_pretrained('SpanBERT/spanbert-base-cased')\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.classifier = nn.Linear(self.spanbert.config.hidden_size, num_labels)\n",
    "        self.crf = nn.CRF(num_labels, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.spanbert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emissions = self.classifier(sequence_output)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = -self.crf(emissions, labels, mask=attention_mask.byte())\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(emissions, mask=attention_mask.byte())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed80258",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpanBERT_CRF(num_labels=num_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avik_2",
   "language": "python",
   "name": "avik_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
