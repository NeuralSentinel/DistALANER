{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8022165d",
   "metadata": {},
   "source": [
    "# ReadMe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5ebf7",
   "metadata": {},
   "source": [
    "1) Copy the commands in 1 to the terminal and execute them\n",
    "2) Run 2 and 3\n",
    "3) Run 4 to update word_to_id.json\n",
    "4) Run 5 to update the labels_so.txt file\n",
    "5) [Run 6 if train/dev/test split is not considered yearwise] or [Run 7 if train/dev/test split is done in a yearwise manner] \n",
    "6) Run 8 in terminal inside the utils_fine_tune directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# execute these before running anything else.\n",
    "! export LD_LIBRARY_PATH=/home/somnath-am/anaconda3/envs/softner/lib/python3.7/site-packages/nvidia/cublas/lib/:$LD_LIBRARY_PATH\n",
    "! export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = '/home/somnath-am/anaconda3/envs/softner/lib/python3.7/site-packages/nvidia/cublas/lib/:$LD_LIBRARY_PATH'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63dbb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "utils_dir = '/home/somnath-am/sr_drive/Avik/NER_Baselines/SoftNER/StackOverflowNER/code/BERT_NER/utils_fine_tune/'\n",
    "csv_file = '/home/somnath-am/sr_drive/Avik/Annotations/results/finalized_dataset/updated_merged_BIO_tagging_removing_verb_adjacent_pos_tags.csv'\n",
    "test_data_dir = \"/home/somnath-am/sr_drive/Avik/Annotations/results/finalized_dataset/human_auto_test_dataset.csv\"\n",
    "test_data_dir_1 = \"/home/somnath-am/sr_drive/Avik/Annotations/results/finalized_dataset/human_auto_test_dataset.csv\"\n",
    "dataset_dir = utils_dir + 'dataset/bio/'\n",
    "\n",
    "data = pd.read_csv(csv_file)\n",
    "# data = data.dropna()\n",
    "data = data.fillna(\"None\")\n",
    "\n",
    "test_data = pd.read_csv(test_data_dir)\n",
    "test_data = test_data.fillna(\"None\")\n",
    "\n",
    "test_data_1 = pd.read_csv(test_data_dir_1)\n",
    "test_data_1 = test_data_1.fillna(\"None\")\n",
    "\n",
    "#data = data.iloc[150000:250000]\n",
    "# to be manually set based on the name of the column in the dataframe\n",
    "bio_tag_str = 'BIO Tag'\n",
    "pos_tag_str = 'POS Tag'\n",
    "word_str = 'Entity'\n",
    "sent_str = 'Bug Id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9adfbd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this for Human Induced dataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "in_data_path = \"/home/somnath-am/sr_drive/Avik/Annotations/results/finalized_dataset_human_induced/\"\n",
    "train_data = pd.read_csv(in_data_path + \"train.csv\")\n",
    "dev_data = pd.read_csv(in_data_path + \"dev.csv\")\n",
    "# test_data = pd.read_csv(in_data_path + \"test.csv\")\n",
    "test_data = pd.read_csv(\"/home/somnath-am/sr_drive/Avik/Annotations/results/finalized_dataset/human_auto_test_dataset.csv\")\n",
    "\n",
    "train_data = train_data.fillna(\"None\")\n",
    "dev_data = dev_data.fillna(\"None\")\n",
    "test_data = test_data.fillna(\"None\")\n",
    "\n",
    "bio_tag_str = 'BIO Tag'\n",
    "pos_tag_str = 'POS Tag'\n",
    "word_str = 'Entity'\n",
    "sent_str = 'Bug Id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb9cbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fea5738a1c4ff3a3cd2a80f56ccb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6fbf789d2d4b76a208a2886887fd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7c8b5eebc3444da7da6c904544cbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Run this for Human Induced dataset\n",
    "\n",
    "train_data['output'] = train_data['Entity'] + '\\t' + train_data['BIO Tag'] + '\\t' + \"CTC_PRED:0\" + \"\\t\" + \"pred_seg_label:O\" + \"\\n\"\n",
    "dev_data['output'] = dev_data['Entity'] + '\\t' + dev_data['BIO Tag'] + '\\t' + \"CTC_PRED:0\" + \"\\t\" + \"pred_seg_label:O\" + \"\\n\"\n",
    "test_data['output'] = test_data['Entity'] + '\\t' + test_data['BIO Tag'] + '\\t' + \"CTC_PRED:0\" + \"\\t\" + \"pred_seg_label:O\" + \"\\n\"\n",
    "\n",
    "out_data_path = \"/home/somnath-am/sr_drive/Avik/NER_Baselines/SoftNER/StackOverflowNER/code/BERT_NER/utils_fine_tune/dataset/bio/ip_ner_human_auto_combined/\"\n",
    "fout_train=open(out_data_path + 'train.txt','w')\n",
    "fout_dev = open(out_data_path + 'dev.txt', 'w')\n",
    "fout_test = open(out_data_path + 'test.txt', 'w')\n",
    "\n",
    "for i,g in tqdm(train_data.groupby('Bug Id')):\n",
    "    for line in g['output'].values:\n",
    "        fout_train.write(line)\n",
    "    fout_train.write('\\n')\n",
    "fout_train.close()\n",
    "\n",
    "for i,g in tqdm(dev_data.groupby('Bug Id')):\n",
    "    for line in g['output'].values:\n",
    "        fout_dev.write(line)\n",
    "    fout_dev.write('\\n')\n",
    "fout_dev.close()\n",
    "\n",
    "for i,g in tqdm(test_data.groupby('Bug Id')):\n",
    "    for line in g['output'].values:\n",
    "        fout_test.write(line)\n",
    "    fout_test.write('\\n')\n",
    "fout_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cb62d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c085c6e2d8004fdca8848692b3fe7f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For Launchpad data\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "test_data = pd.read_csv(\"/home/somnath-am/sr_drive/Avik/Launchpad/launchpad_bio_tagging_human.csv\")\n",
    "# test_data = pd.read_csv(\"/home/somnath-am/sr_drive/Avik/Annotations/results/finalized_dataset/human_auto_test_dataset.csv\")\n",
    "test_data = test_data.fillna(\"None\")\n",
    "bio_tag_str = 'BIO Tag'\n",
    "pos_tag_str = 'POS Tag'\n",
    "word_str = 'Entity'\n",
    "sent_str = 'Bug Id'\n",
    "\n",
    "test_data['output'] = test_data['Entity'] + '\\t' + test_data['BIO Tag'] + '\\t' + \"CTC_PRED:0\" + \"\\t\" + \"pred_seg_label:O\" + \"\\n\"\n",
    "out_data_path = \"/home/somnath-am/sr_drive/Avik/NER_Baselines/SoftNER/StackOverflowNER/code/BERT_NER/utils_fine_tune/dataset/bio/launchpad/\"\n",
    "fout_test = open(out_data_path + 'test.txt', 'w')\n",
    "for i,g in tqdm(test_data.groupby('Bug Id')):\n",
    "    for line in g['output'].values:\n",
    "        fout_test.write(line)\n",
    "    fout_test.write('\\n')\n",
    "fout_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac1b694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d862f607a3f42a5b849bc5e9191663a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0d1e6427db47d8919c7c981a25bd7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ab498053e04731920b2d06e4e89c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3\n",
    "# for creating the data file which is in (word --> BIO tag) format\n",
    "data = data[[sent_str, word_str, bio_tag_str]]\n",
    "\n",
    "# grp_len = data.groupby(sent_str).size()\n",
    "# excess_len = []\n",
    "# for i in grp_len.index:\n",
    "#     if grp_len.loc[i]>=300:\n",
    "#         excess_len.append(i)\n",
    "# data =  data[~data[sent_str].isin(excess_len)]\n",
    "\n",
    "def conll_format(df, op_file, file_type):\n",
    "    with open(op_file, file_type) as fout:\n",
    "        df['join'] = df['Entity'] + '\\t' + df['BIO Tag']\n",
    "        for i, g in tqdm(df.groupby('Bug Id')['join']):\n",
    "            for line in g.values:\n",
    "                print(line, file = fout)\n",
    "            print(file = fout)\n",
    "    return\n",
    "\n",
    "conll_format(data, dataset_dir+'data.txt', 'w')\n",
    "conll_format(test_data, dataset_dir+'data.txt', 'a')\n",
    "conll_format(test_data_1, dataset_dir+'data.txt', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a41d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "train_yrs = list(range(2004, 2014))\n",
    "valid_yrs = list(range(2014, 2016))\n",
    "test_yrs = list(range(2016, 2020))\n",
    "\n",
    "out_dir = 'utils_fine_tune/dataset/bio/ip_ner/'\n",
    "fout_train=open(out_dir + 'train.txt','w')\n",
    "fout_dev = open(out_dir + 'dev.txt', 'w')\n",
    "fout_test = open(out_dir + 'test.txt', 'w')\n",
    "\n",
    "data['output'] = data['Entity'] + '\\t' + data['BIO Tag'] + '\\t' + \"CTC_PRED:0\" + \"\\t\" + \"pred_seg_label:O\" + \"\\n\"\n",
    "test_data['output'] = test_data['Entity'] + '\\t' + test_data['BIO Tag'] + '\\t' + \"CTC_PRED:0\" + \"\\t\" + \"pred_seg_label:O\" + \"\\n\"\n",
    "test_data_1['output'] = test_data_1['Entity'] + '\\t' + test_data_1['BIO Tag'] + '\\t' + \"CTC_PRED:0\" + \"\\t\" + \"pred_seg_label:O\" + \"\\n\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff5a6696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd49c1e95864fe980dd6bac5c3c9213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fout_dev = open(out_dir + 'dev.txt', 'w')\n",
    "# for i,g in tqdm(data.groupby('Bug Id')):\n",
    "#     if g['year'].values[0] in valid_yrs:\n",
    "#         for line in g['output'].values:\n",
    "#             if len(line.split()) != 4 and len(line.split()) != 0 : \n",
    "#                 print(line)\n",
    "#             else:\n",
    "#                 fout_dev.write(line)\n",
    "#         fout_dev.write('\\n')\n",
    "# fout_dev.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "744ee55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = 'utils_fine_tune/dataset/bio/ip_ner/test.txt'  # Path to the .txt file\n",
    "# z = []\n",
    "# # Open the file in read mode\n",
    "# with open(file_path, 'r') as file:\n",
    "#     # Read each line and process it\n",
    "#     for line in file:\n",
    "#         # Process the line as a string\n",
    "#         line = line.strip()  # Remove leading/trailing whitespaces\n",
    "#         # Perform any desired operations with the line\n",
    "#         lst = line.split()\n",
    "#         z.append(lst)\n",
    "# p = pd.DataFrame(z, columns = ['a', 'b', 'c', 'd'])\n",
    "# p['b'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef42d934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24e770e0ad14f6db0260124e4fac1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79bec67ee704f13a83eb1ad42d9f625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i,g in tqdm(data.groupby('Bug Id')):\n",
    "    if g['year'].values[0] in train_yrs:\n",
    "        for line in g['output'].values:\n",
    "            fout_train.write(line)\n",
    "        fout_train.write('\\n')\n",
    "    elif g['year'].values[0] in valid_yrs:\n",
    "        for line in g['output'].values:\n",
    "            if len(line.split()) != 4 and len(line.split()) != 0 : \n",
    "                print(line)\n",
    "            else:\n",
    "                fout_dev.write(line)\n",
    "        fout_dev.write('\\n')\n",
    "#     else:\n",
    "#         for line in g['output'].values:\n",
    "#             fout_test.write(line)\n",
    "#         fout_test.write('\\n')\n",
    "        \n",
    "fout_train.close()\n",
    "fout_dev.close()\n",
    "\n",
    "for i,g in tqdm(test_data.groupby('Bug Id')):\n",
    "    for line in g['output'].values:\n",
    "        fout_test.write(line)\n",
    "    fout_test.write('\\n')\n",
    "\n",
    "fout_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab1029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "# for creating the \"word_to_id.json\" file for the input data\n",
    "word_to_id = {}\n",
    "word_id = 0\n",
    "word_to_id['***PADDING***'] = word_id\n",
    "word_id += 1\n",
    "word_to_id['UNK'] = word_id\n",
    "word_id += 1\n",
    "for line in open(dataset_dir+\"data.txt\"):\n",
    "    if line.strip() == \"\":\n",
    "        continue\n",
    "    word = line.split()[0]\n",
    "    if word in word_to_id.keys():\n",
    "        continue\n",
    "    word_to_id[word] = word_id\n",
    "    word_id += 1\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "file_path = utils_dir + \"word_to_id.json\"\n",
    "if os.path.isfile(file_path):\n",
    "    os.remove(file_path)\n",
    "with open(file_path, \"w\") as fp:\n",
    "    json.dump(word_to_id,fp)\n",
    "\n",
    "file_path = \"/home/somnath-am/sr_drive/Avik/NER_Baselines/SoftNER/StackOverflowNER/code/BERT_NER/word_to_id.json\"\n",
    "if os.path.isfile(file_path):\n",
    "    os.remove(file_path)\n",
    "with open(file_path, \"w\") as fp:\n",
    "    json.dump(word_to_id,fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b96a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# creates labels for the dataset\n",
    "fout = open('utils_fine_tune/labels_so.txt', 'w')\n",
    "labels_arr = data[bio_tag_str].unique()\n",
    "entity_set = set([label[2:] for label in labels_arr if label!='O'])\n",
    "entity_arr = [\"B-\"+label for label in entity_set] + [\"I-\"+label for label in entity_set]\n",
    "entity_arr.append('O')\n",
    "for label in entity_arr:\n",
    "    fout.write(label + '\\n')\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d5d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for creating the data.txt file which is in (word -----> BIO tag) format\n",
    "# text_data = ''\n",
    "# with open('/home/somnath-am/HULK/HULK/Avik/NER_Baselines/adaseq/dataset/bio/train.txt', 'r') as fout:\n",
    "#     data = fout.read()\n",
    "#     text_data = text_data + data\n",
    "# fout.close()\n",
    "# with open('/home/somnath-am/HULK/HULK/Avik/NER_Baselines/adaseq/dataset/bio/dev.txt', 'r') as fout:\n",
    "#     data = fout.read()\n",
    "#     text_data = text_data + data\n",
    "# fout.close()\n",
    "# with open('/home/somnath-am/HULK/HULK/Avik/NER_Baselines/adaseq/dataset/bio/test.txt', 'r') as fout:\n",
    "#     data = fout.read()\n",
    "#     text_data = text_data + data\n",
    "# #text_data = text_data.replace('\\n\\n', '\\n')\n",
    "# fout.close()\n",
    "# text_file = open(\"/home/somnath-am/HULK/HULK/Avik/NER_Baselines/SoftNER/StackOverflowNER/code/BERT_NER/utils_fine_tune/dataset/bio/data.txt\", \"w\")\n",
    "# text_file.write(text_data)\n",
    "# text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ip_seg_format(ip_file, op_file):\n",
    "    fout=open(op_file,'w')\n",
    "    for line in open(ip_file):\n",
    "        if line.strip()==\"\":\n",
    "            fout.write(\"\\n\")\n",
    "            continue\n",
    "        line_values = line.strip().split()\n",
    "        if len(line_values)!=2:\n",
    "            print(line)\n",
    "            continue\n",
    "        else:\n",
    "            word, md = line.split()\n",
    "        if md!=\"O\":\n",
    "            opline = word+\"\\t\"+ \"B-Name\" +\"\\t\"+\"CTC_PRED:0\"+\"\\t\"+\"md_label:Name\"+\"\\n\"\n",
    "        else :\n",
    "            opline = word+\"\\t\"+ \"O\" +\"\\t\"+\"CTC_PRED:0\"+\"\\t\"+\"md_label:O\"+\"\\n\"\n",
    "        # print(opline)\n",
    "        fout.write(opline)\n",
    "    fout.close()\n",
    "    return\n",
    "#create_ip_seg_format(data_dir + 'train.txt', 'train')\n",
    "#create_ip_seg_format(data_dir + 'dev.txt', 'dev')\n",
    "#create_ip_seg_format(data_dir + 'test.txt', 'test')\n",
    "create_ip_seg_format(dataset_dir + \"data.txt\", dataset_dir + 'ip_seg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11884d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# go inside utils_fine_tune folder and run the command below\n",
    "# python softner_segmenter.py --data_dir dataset/bio/ip_seg/ --do_train --do_eval --do_predict  --overwrite_output_dir \n",
    "\n",
    "# execute this inside the BERT_NER folder -- *DO THIS*\n",
    "! python softner_segmenter_preditct_from_file.py --input_file_for_segmenter utils_fine_tune/dataset/bio/ip_seg.txt --output_file_for_segmenter utils_fine_tune/dataset/bio/segmenter_preds.txt --per_gpu_train_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403fe166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "from tqdm.auto import tqdm\n",
    "def create_ner_ip_format(ip_file, out_dir):\n",
    "    \n",
    "    fout_train=open(out_dir + 'train.txt','w')\n",
    "    fout_dev = open(out_dir + 'dev.txt', 'w')\n",
    "    fout_test = open(out_dir + 'test.txt', 'w')\n",
    "    \n",
    "    with open(ip_file, 'r') as f_inp:\n",
    "        size = len(f_inp.readlines())\n",
    "    train_idx=int(round(0.8*size))\n",
    "    dev_test_size = size-train_idx\n",
    "    dev_idx = int(round(dev_test_size/2)) + train_idx\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for line in tqdm(open(ip_file)):\n",
    "        if line.strip()==\"\":\n",
    "            if idx < train_idx:\n",
    "                fout_train.write('\\n')\n",
    "            elif idx < dev_idx:\n",
    "                fout_dev.write('\\n')\n",
    "            else:\n",
    "                fout_test.write('\\n')\n",
    "            idx+=1\n",
    "            continue\n",
    "\n",
    "        # print(\"--------------\",line)\n",
    "\n",
    "        line_values = line.strip().split()\n",
    "        if len(line_values)!=2:\n",
    "            print(line)\n",
    "            continue\n",
    "        else:\n",
    "            word, tag = line.strip().split()\n",
    "            \n",
    "        opline = word+\"\\t\"+tag+\"\\t\"+\"CTC_PRED:0\"+\"\\t\"+\"pred_seg_label:O\"+\"\\n\"\n",
    "        # print(opline)\n",
    "        if idx < train_idx:\n",
    "            fout_train.write(opline)\n",
    "        elif idx < dev_idx:\n",
    "            fout_dev.write(opline)\n",
    "        else:\n",
    "            fout_test.write(opline)\n",
    "        idx += 1\n",
    "        \n",
    "    fout_train.close()\n",
    "    fout_dev.close()\n",
    "    fout_test.close()\n",
    "\n",
    "create_ner_ip_format('utils_fine_tune/dataset/bio/data.txt', 'utils_fine_tune/dataset/bio/ip_ner/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54994813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "from tqdm.auto import tqdm\n",
    "def create_ner_ip_format_yearwise(ip_file, out_dir):\n",
    "    \n",
    "    fout_train=open(out_dir + 'train.txt','w')\n",
    "    fout_dev = open(out_dir + 'dev.txt', 'w')\n",
    "    fout_test = open(out_dir + 'test.txt', 'w')\n",
    "    \n",
    "    with open(ip_file, 'r') as f_inp:\n",
    "        size = len(f_inp.readlines())\n",
    "    train_idx=int(round(0.8*size))\n",
    "    dev_test_size = size-train_idx\n",
    "    dev_idx = int(round(dev_test_size/2)) + train_idx\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for line in tqdm(open(ip_file)):\n",
    "        if line.strip()==\"\":\n",
    "            if idx < train_idx:\n",
    "                fout_train.write('\\n')\n",
    "            elif idx < dev_idx:\n",
    "                fout_dev.write('\\n')\n",
    "            else:\n",
    "                fout_test.write('\\n')\n",
    "            idx+=1\n",
    "            continue\n",
    "\n",
    "        # print(\"--------------\",line)\n",
    "\n",
    "        line_values = line.strip().split()\n",
    "        if len(line_values)!=2:\n",
    "            print(line)\n",
    "            continue\n",
    "        else:\n",
    "            word, tag = line.strip().split()\n",
    "            \n",
    "        opline = word+\"\\t\"+tag+\"\\t\"+\"CTC_PRED:0\"+\"\\t\"+\"pred_seg_label:O\"+\"\\n\"\n",
    "        # print(opline)\n",
    "        if idx < train_idx:\n",
    "            fout_train.write(opline)\n",
    "        elif idx < dev_idx:\n",
    "            fout_dev.write(opline)\n",
    "        else:\n",
    "            fout_test.write(opline)\n",
    "        idx += 1\n",
    "        \n",
    "    fout_train.close()\n",
    "    fout_dev.close()\n",
    "    fout_test.close()\n",
    "\n",
    "create_ner_ip_format_yearwise('utils_fine_tune/dataset/bio/data.txt', 'utils_fine_tune/dataset/bio/ip_ner/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the \"_num_labels\" and other parameters inside utils_fine_tune/bert-word-piece-softner/config.json based on the number of labels in the dataset (num + 4)\n",
    "# change the parameters inside utils_fine_tune/word_piece_ner/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6eeb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "#! python softner_ner_predict_from_file.py --input_file_for_ner dataset/ner_ip.txt --output_file_for_ner dataset/ner_preds.txt\n",
    "# inside utils_fine_tune\n",
    "!python softner_ner.py --data_dir dataset/bio/ip_ner/ --do_train --do_eval --do_predict --overwrite_output_dir --save_steps 5000 --max_seq_length 512 --overwrite_cache --num_train_epochs 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "csv_file = '/home/somnath-am/HULK/HULK/Avik/NER_Baselines/updated_BIO_tagging_removing_verb_adjacent_pos_tags.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0daa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.groupby('Bug Id').size()\n",
    "x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd98732",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf31475",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500a7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['join'] = df['Entity'] + ' ' + df['BIO Tag']\n",
    "for i, g in tqdm(df.groupby('Bug Id')['join']):\n",
    "    for line in g.values:\n",
    "        print(line, file = fout)\n",
    "    print(file = fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('utils_fine_tune/freq_embeds.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b939c8",
   "metadata": {},
   "source": [
    "# Segmenter Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ['LD_LIBRARY_PATH'] = '/home/somnath-am/anaconda3/envs/softner/lib/python3.7/site-packages/nvidia/cublas/lib/:$LD_LIBRARY_PATH'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "dataset_dir = '/home/somnath-am/HULK/HULK/Avik/NER_Baselines/SoftNER/StackOverflowNER/code/BERT_NER/utils_fine_tune/dataset/bio/'\n",
    "data_dir = dataset_dir + 'data/'\n",
    "seg_dir = dataset_dir + 'ip_seg/'\n",
    "\n",
    "csv_file = '/home/somnath-am/HULK/HULK/Avik/NER_Baselines/updated_BIO_tagging_removing_verb_adjacent_pos_tags.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.fillna(method=\"ffill\")\n",
    "# to be manually set based on the name of the column in the dataframe\n",
    "bio_tag_str = 'BIO Tag'\n",
    "pos_tag_str = 'POS Tag'\n",
    "word_str = 'Entity'\n",
    "sent_str = 'Bug Id'\n",
    "\n",
    "def conll_format(df, op_file):\n",
    "    with open(op_file, 'w') as fout:\n",
    "        df['join'] = df[word_str] + '\\t' + df[bio_tag_str]\n",
    "        for i, g in tqdm(df.groupby(sent_str)['join']):\n",
    "            for line in g.values:\n",
    "                print(line, file = fout)\n",
    "            print(file = fout)\n",
    "    return\n",
    "\n",
    "def create_ip_seg_format(ip_file, op_file):\n",
    "    fout=open(op_file,'w')\n",
    "    for line in open(ip_file):\n",
    "        if line.strip()==\"\":\n",
    "            fout.write(\"\\n\")\n",
    "            continue\n",
    "        line_values = line.strip().split()\n",
    "        if len(line_values)!=2:\n",
    "            print(line)\n",
    "            continue\n",
    "        else:\n",
    "            word, md = line.split()\n",
    "        if md!=\"O\":\n",
    "            opline = word+\"\\t\"+ \"B-Name\" +\"\\t\"+\"CTC_PRED:0\"+\"\\t\"+\"md_label:Name\"+\"\\n\"\n",
    "        else :\n",
    "            opline = word+\"\\t\"+ \"O\" +\"\\t\"+\"CTC_PRED:0\"+\"\\t\"+\"md_label:O\"+\"\\n\"\n",
    "        # print(opline)\n",
    "        fout.write(opline)\n",
    "    fout.close()\n",
    "    return\n",
    "\n",
    "data_len = df.shape[0]\n",
    "window_len = 130000\n",
    "num_epochs = 5\n",
    "i=0\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for j in range(window_len,data_len,window_len):\n",
    "\n",
    "        file_path = dataset_dir + 'data.txt'\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        file_path = \"/home/somnath-am/HULK/HULK/Avik/NER_Baselines/SoftNER/StackOverflowNER/code/BERT_NER/word_to_id.json\"\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        file_path = dataset_dir + 'ip_seg.txt'\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "\n",
    "        data = df.iloc[i:j]\n",
    "        data = data[[sent_str, word_str, bio_tag_str]]\n",
    "        conll_format(data, dataset_dir+'data.txt')\n",
    "\n",
    "        word_to_id = {}\n",
    "        word_id = 0\n",
    "        word_to_id['***PADDING***'] = word_id\n",
    "        word_id += 1\n",
    "        word_to_id['UNK'] = word_id\n",
    "        word_id += 1\n",
    "        for line in open(dataset_dir + \"data.txt\"):\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            word = line.split()[0]\n",
    "            if word in word_to_id.keys():\n",
    "                continue\n",
    "            word_to_id[word] = word_id\n",
    "            word_id += 1\n",
    "        import json\n",
    "        with open(\"/home/somnath-am/HULK/HULK/Avik/NER_Baselines/SoftNER/StackOverflowNER/code/BERT_NER/word_to_id.json\", \"w\") as fp:\n",
    "            json.dump(word_to_id,fp) \n",
    "\n",
    "        create_ip_seg_format(dataset_dir + \"data.txt\", dataset_dir + 'ip_seg.txt')\n",
    "        ! python softner_segmenter_preditct_from_file.py --input_file_for_segmenter utils_fine_tune/dataset/bio/ip_seg.txt --output_file_for_segmenter utils_fine_tune/dataset/bio/segmenter_preds.txt --num_train_epochs 1\n",
    "        \n",
    "        i=j\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d2a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d35a7953",
   "metadata": {},
   "source": [
    "#### Do not run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e65dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfada042d7e140ca88d5985a9890384e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "p = []\n",
    "with open('utils_fine_tune/dataset/bio/ip_ner_human_induced/test.txt', 'r') as file:\n",
    "    # Read the file line by line\n",
    "    for line in tqdm(file):\n",
    "        # Process each line (print it in this example)\n",
    "        k = len(line.strip().split())\n",
    "        if k != 4 and k != 0:\n",
    "            print(line)\n",
    "            continue\n",
    "        p.append(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c02ca48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a09427780224b909b209fdca5bbb70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5485324 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fout_dev = open(out_data_path + 'dev.txt', 'w')\n",
    "for line in tqdm(p):\n",
    "    fout_dev.write(line)\n",
    "fout_dev.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b71a588a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28b45d916af47acb08e0913cf9c1924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "with open('utils_fine_tune/dataset/bio/ip_ner/dev.txt', 'r') as file:\n",
    "    # Read the file line by line\n",
    "    for line in tqdm(file):\n",
    "        # Process each line (print it in this example)\n",
    "        k = len(line.strip().split())\n",
    "        if k != 4 and k != 0:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b079e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[3822367]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d55729",
   "metadata": {},
   "source": [
    "#### Calculating accuracy of model on human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60696d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d2f3f07da94fcc91afc18f3e297a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "y_pred_lst = []\n",
    "y_pred_entry = []\n",
    "with open('utils_fine_tune/only_human_model/test_predictions.txt', 'r') as file:\n",
    "    # Read the file line by line\n",
    "    for line in tqdm(file):\n",
    "        # Process each line (print it in this example)\n",
    "        k = line.strip().split()\n",
    "        if len(k) == 0:\n",
    "            y_pred_lst.append(y_pred_entry)\n",
    "            y_pred_entry = []\n",
    "        else:\n",
    "            y_pred_entry.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f25afef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699577f46ffd47fab7ca464c6c08af36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_lst = []\n",
    "y_test_entry = []\n",
    "with open('utils_fine_tune/dataset/bio/launchpad/test.txt', 'r') as file:\n",
    "    # Read the file line by line\n",
    "    for line in tqdm(file):\n",
    "        # Process each line (print it in this example)\n",
    "        k = line.strip().split()\n",
    "        if len(k) == 0:\n",
    "            y_test_lst.append(y_test_entry)\n",
    "            y_test_entry = []\n",
    "        else:\n",
    "            y_test_entry.append(k[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb1f5aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classwise accuracy : \n",
      "{'Package': 0.0, 'Software_Component': 0.0, 'Command': 0.0, 'OS': 0.0, 'Peripheral': 0.0, 'Architecture': 0.0, 'Organization': 0.0, 'Extension': 0.0, 'Error': 0.0, 'g': 0.0}\n",
      "The overall accuracy : 0.0\n"
     ]
    }
   ],
   "source": [
    "classwise_acc_dict = {}\n",
    "classwise_count_dict = {}\n",
    "for i in range(len(y_pred_lst)):\n",
    "    for j in range(len(y_pred_lst[i])):\n",
    "        if y_test_lst[i][j][-1] != 'O':\n",
    "            ent = y_test_lst[i][j][-1][2:]\n",
    "            \n",
    "            if ent not in classwise_count_dict:\n",
    "                classwise_count_dict[ent] = 0\n",
    "            classwise_count_dict[ent] = classwise_count_dict[ent] + 1\n",
    "            \n",
    "            if ent not in classwise_acc_dict:\n",
    "                classwise_acc_dict[ent] = 0\n",
    "            if y_pred_lst[i][j][-1][2:] == ent:\n",
    "                classwise_acc_dict[ent] = classwise_acc_dict[ent] + 1\n",
    "sum_correct = 0\n",
    "for key in classwise_acc_dict.keys():\n",
    "    sum_correct += classwise_acc_dict[key]\n",
    "    classwise_acc_dict[key] = classwise_acc_dict[key]/classwise_count_dict[key]\n",
    "print(\"The classwise accuracy : \")\n",
    "print(classwise_acc_dict)\n",
    "overall_accuracy = sum_correct/sum(classwise_count_dict.values())\n",
    "print(f\"The overall accuracy : {overall_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4964871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reproduces', 'O']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_lst[0][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b65e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  9 09:45:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   55C    P0   154W / 250W |  10003MiB / 12198MiB |     96%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  Off  | 00000000:DB:00.0 Off |                    0 |\n",
      "| N/A   51C    P0   171W / 250W |  15423MiB / 16280MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1478      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    930151      C   python                           9997MiB |\n",
      "|    1   N/A  N/A      1478      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A    923447      C   python                          15417MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26d9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softner",
   "language": "python",
   "name": "softner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
