{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb2dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "import seqeval\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b02c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_seed = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed(global_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6157b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "def get_device(device_id = 0):\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(f\"cuda:{device_id}\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(device_id))\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "\n",
    "device = get_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff4b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f59f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 768\n",
    "HIDDEN_DIM = 256\n",
    "MINIBATCH_SIZE = 2\n",
    "LEARNING_WEIGHT = 5e-2 #learning rate\n",
    "WEIGHT_DECAY = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d79dca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1).to(device)\n",
    "        tmp = torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long)\n",
    "#         print(tmp.is_cuda)\n",
    "#         tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags]).to(device)\n",
    "        tags = torch.cat([tmp.to(device), tags]).to(device)\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1).to(device)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7da7799",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\somii/sr_drive/Avik/Annotations/results/finalized_dataset/updated_merged_BIO_tagging_removing_verb_adjacent_pos_tags.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10940/4093945250.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#data_df = pd.read_csv('BIO_tagging.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"~/sr_drive/Avik/Annotations/results/finalized_dataset/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"updated_merged_BIO_tagging_removing_verb_adjacent_pos_tags.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"human_test_dataset.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtest_data_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"human_auto_test_dataset.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 )\n\u001b[1;32m--> 331\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1735\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\somii/sr_drive/Avik/Annotations/results/finalized_dataset/updated_merged_BIO_tagging_removing_verb_adjacent_pos_tags.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#data_df = pd.read_csv('BIO_tagging.csv')\n",
    "data_path = \"~/sr_drive/Avik/Annotations/results/finalized_dataset/\"\n",
    "data_df = pd.read_csv(data_path + \"updated_merged_BIO_tagging_removing_verb_adjacent_pos_tags.csv\")\n",
    "test_data = pd.read_csv(data_path + \"human_test_dataset.csv\")\n",
    "test_data_1 = pd.read_csv(data_path + \"human_auto_test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b876d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this for Human Induced Dataset\n",
    "import pandas as pd\n",
    "data_path = \"G:/my Drive/Avik/Annotations/results/finalized_dataset_human_induced/\"\n",
    "train_data = pd.read_csv(data_path + \"train.csv\")\n",
    "dev_data = pd.read_csv(data_path + \"dev.csv\")\n",
    "test_data = pd.read_csv(data_path + \"test.csv\")\n",
    "# test_data = pd.read_csv(\"G:/My Drive/Avik/Launchpad/launchpad_bio_tagging_human.csv\")\n",
    "\n",
    "train_data = train_data.fillna(\"None\")\n",
    "dev_data = dev_data.fillna(\"None\")\n",
    "test_data = test_data.fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "202d1913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a314770e7f9b48a3814f9b6f7ec5d4bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_to_ix = {}\n",
    "for idx, row in tqdm(train_data.iterrows()):\n",
    "    if(row['Entity'] not in word_to_ix):\n",
    "        word_to_ix[row['Entity']] = len(word_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acae0d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44c1961330b403191199fc88ed79a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eac77b2e1cd474d8ea6bf2b8a7a85da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, row in tqdm(dev_data.iterrows()):\n",
    "    if(row['Entity'] not in word_to_ix):\n",
    "        word_to_ix[row['Entity']] = len(word_to_ix)\n",
    "for idx, row in tqdm(test_data.iterrows()):\n",
    "    if(row['Entity'] not in word_to_ix):\n",
    "        word_to_ix[row['Entity']] = len(word_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0865196",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_to_word = {idx: word for word, idx in word_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7224185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.5 0\n",
      "on 1\n",
      "RH8.0 2\n",
      "i 3\n",
      "set 4\n",
      "the 5\n",
      "terse 6\n",
      "phase 7\n",
      "via 8\n",
      "web 9\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "for k,v in sorted(word_to_ix.items(), key=operator.itemgetter(1))[:10]:\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc43ff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the word Ubuntu is 2881.\n",
      "The word with index 2881 is Ubuntu.\n"
     ]
    }
   ],
   "source": [
    "test_word = \"Ubuntu\"\n",
    "\n",
    "test_word_idx = word_to_ix[test_word]\n",
    "test_word_lookup = ix_to_word[test_word_idx]\n",
    "\n",
    "print(\"The index of the word {} is {}.\".format(test_word, test_word_idx))\n",
    "print(\"The word with index {} is {}.\".format(test_word_idx, test_word_lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9210d01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-Organization': 1, 'B-Package': 2, 'B-Command': 3, 'B-Software_Component': 4, 'B-OS': 5, 'B-Error': 6, 'I-Error': 7, 'B-Extension': 8, 'I-OS': 9, 'B-Peripheral': 10, 'B-Architecture': 11, 'I-Package': 12, 'I-Software_Component': 13, 'I-Extension': 14, 'I-Peripheral': 15, 'I-Architecture': 16, 'I-Command': 17, 'I-Organization': 18, '<START>': 19, '<STOP>': 20}\n"
     ]
    }
   ],
   "source": [
    "tag_to_ix = {}\n",
    "for idx, row in train_data.iterrows():\n",
    "    if(row['BIO Tag'] not in tag_to_ix):\n",
    "        tag_to_ix[row['BIO Tag']] = len(tag_to_ix)\n",
    "tag_to_ix[START_TAG] = len(tag_to_ix)\n",
    "tag_to_ix[STOP_TAG] = len(tag_to_ix)\n",
    "\n",
    "print(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb552d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-Organization', 2: 'B-Package', 3: 'B-Command', 4: 'B-Software_Component', 5: 'B-OS', 6: 'B-Error', 7: 'I-Error', 8: 'B-Extension', 9: 'I-OS', 10: 'B-Peripheral', 11: 'B-Architecture', 12: 'I-Package', 13: 'I-Software_Component', 14: 'I-Extension', 15: 'I-Peripheral', 16: 'I-Architecture', 17: 'I-Command', 18: 'I-Organization', 19: '<START>', 20: '<STOP>'}\n"
     ]
    }
   ],
   "source": [
    "ix_to_tag = {idx: tag for tag, idx in tag_to_ix.items()}\n",
    "print(ix_to_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a9f6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_WEIGHT, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a0983b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somii\\AppData\\Local\\Temp/ipykernel_10940/122929333.py:10: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  train_data_group = train_data.groupby(\n",
      "C:\\Users\\somii\\AppData\\Local\\Temp/ipykernel_10940/122929333.py:14: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  valid_data_group = valid_data.groupby(\n",
      "C:\\Users\\somii\\AppData\\Local\\Temp/ipykernel_10940/122929333.py:18: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  test_data_group = test_data.groupby(\n"
     ]
    }
   ],
   "source": [
    "# train_yrs = list(range(2004, 2014))\n",
    "# valid_yrs = list(range(2014, 2016))\n",
    "# # test_yrs = list(range(2016, 2020))\n",
    "\n",
    "# train_data = data_df[data_df['year'].isin(train_yrs)]\n",
    "# valid_data = data_df[data_df['year'].isin(valid_yrs)]\n",
    "# test_data = data_df[data_df['year'].isin(test_yrs)]\n",
    "valid_data = dev_data\n",
    "\n",
    "train_data_group = train_data.groupby(\n",
    "['Bug Id'],as_index=False\n",
    ")['Entity', 'POS Tag', 'BIO Tag'].agg(lambda x: list(x))\n",
    "\n",
    "valid_data_group = valid_data.groupby(\n",
    "['Bug Id'],as_index=False\n",
    ")['Entity', 'POS Tag', 'BIO Tag'].agg(lambda x: list(x))\n",
    "\n",
    "test_data_group = test_data.groupby(\n",
    "['Bug Id'],as_index=False\n",
    ")['Entity', 'POS Tag', 'BIO Tag'].agg(lambda x: list(x))\n",
    "\n",
    "training_data = []\n",
    "for idx, row in train_data_group.iterrows():\n",
    "    training_data.append((row['Entity'], row['BIO Tag']))\n",
    "    \n",
    "validating_data = []\n",
    "for idx, row in valid_data_group.iterrows():\n",
    "    validating_data.append((row['Entity'], row['BIO Tag']))\n",
    "    \n",
    "testing_data = []\n",
    "for idx, row in test_data_group.iterrows():\n",
    "    testing_data.append((row['Entity'], row['BIO Tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_group_1 = test_data_1.groupby(\n",
    "['Bug Id'],as_index=False\n",
    ")['Entity', 'POS Tag', 'BIO Tag'].agg(lambda x: list(x))\n",
    "\n",
    "testing_data_1 = []\n",
    "for idx, row in test_data_group.iterrows():\n",
    "    testing_data.append((row['Entity'], row['BIO Tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_group = data_df.groupby(\n",
    "# ['Bug Id'],as_index=False\n",
    "# )['Entity', 'POS Tag', 'BIO Tag'].agg(lambda x: list(x))\n",
    "\n",
    "# data_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_data = []\n",
    "# for idx, row in data_group.iterrows():\n",
    "#     formatted_data.append((row['Entity'], row['BIO Tag']))\n",
    "# print(formatted_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c270d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train test split in terms of year\n",
    "# formatted_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f604de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the training dataset: 64787\n",
      "Number of sentences in the validation dataset: 99\n",
      "Number of sentences in the test dataset : 351\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# training_data, testing_data = train_test_split(formatted_data, test_size=0.2, random_state=1234)\n",
    "# training_data, validating_data = train_test_split(training_data, test_size=0.25, random_state=1234)\n",
    "\n",
    "print(\"Number of sentences in the training dataset: {}\".format(len(training_data)))\n",
    "print(\"Number of sentences in the validation dataset: {}\".format(len(validating_data)))\n",
    "print(\"Number of sentences in the test dataset : {}\".format(len(testing_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0398d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_sent = precheck_sent.to(device)\n",
    "    pred =  model(precheck_sent)[1]\n",
    "    print('Prediction:   ', [ix_to_tag[idx] for idx in pred])\n",
    "    print('Ground truth: ', training_data[0][1])\n",
    "    print(training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6310c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7da26379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa6ca3213694c75abd006193b549d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d326d2cc16647d492266d40392fa966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 89.0332224792879\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "losses = []\n",
    "for epoch in tqdm(range(1)):  # 30 epochs used before \n",
    "    for sentence, tags in tqdm(training_data):\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance of LSTM\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
    "        sentence_in, targets = sentence_in.to(device), targets.to(device)\n",
    "\n",
    "\n",
    "        \n",
    "        # Step 3. A lot happens.  Run our forward pass to get features from BLSTM,\n",
    "        # run the CRF and get the negative log likelihoods and find the best \n",
    "        # \"path\" through sentence with the tags using the viterbi algorithm \n",
    "        # (also part of forward pass).\n",
    "        # BTW our dynamic computational graph is created with the forward pass\n",
    "        # Returns the forward score - ground truth score (our loss measure)\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "\n",
    "        # Step 4. Compute the loss, gradients (backprop), and update the \n",
    "        # parameters by calling optimizer.step() - optimizer here is \n",
    "        # SGD for our CRF\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch) % 10 == 0:\n",
    "        print(\"Epoch: {} Loss: {}\".format(epoch+1, np.mean(losses)))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "# with open('BiLSTM_runtimes.txt') as fout:\n",
    "#     fout.write(f'Runtime of the program is: {end - start} seconds')\n",
    "\n",
    "#torch.save(model.state_dict(), 'bilstm_crf.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(losses)\n",
    "print(len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71daada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(losses)), losses)\n",
    "print(\"Loss value are training : {}\".format(losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35440c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     postcheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "#     pred =  model(postcheck_sent)[1]\n",
    "#     print('Prediction:   ', [ix_to_tag[idx] for idx in pred])\n",
    "#     print('Ground truth: ', training_data[0][1])\n",
    "#     print(training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f645b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from seqeval.metrics import classification_report\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def print_f1_score(y_label, y_pred):\n",
    "    y_label = list(np.concatenate(y_label).flat)\n",
    "    y_pred = list(np.concatenate(y_pred).flat)\n",
    "    print('Micro f1 score: ', f1_score(y_label, y_pred, average='micro'))\n",
    "    print('Macro f1 score: ', f1_score(y_label, y_pred, average='macro'))\n",
    "    print('Weighted f1 score: ', f1_score(y_label, y_pred, average='weighted'))\n",
    "\n",
    "def print_classwise_f1_score(y_label, y_pred, fout = None):\n",
    "    y_label = list(np.concatenate(y_label).flat)\n",
    "    y_pred = list(np.concatenate(y_pred).flat)\n",
    "    labels=['B-Command', 'B-Error_description', 'B-Error_name', 'B-Extension',\n",
    "       'B-OS', 'B-Package', 'B-URL', 'B-Ubuntu_distribution',\n",
    "       'I-Error_description', 'I-Extension', 'I-URL', 'O']\n",
    "    classwise_f1_score = f1_score(y_label, y_pred, average=None, labels=labels)\n",
    "    if fout == None:\n",
    "        print('Classwise F-1 Score')\n",
    "    else:\n",
    "        fout.write('Classwise F-1 Score\\n')\n",
    "    for label,score in zip(labels, classwise_f1_score):\n",
    "        if fout == None:\n",
    "            print(str(label + ' : ' + str(score)))\n",
    "        else:\n",
    "            fout.write(str(label + ' : ' + str(score) + '\\n'))\n",
    "    if fout == None:\n",
    "        print()\n",
    "    else:\n",
    "        fout.write('\\n')\n",
    "\n",
    "def print_classification_report(y_label, y_pred):\n",
    "    #y_label = list(np.concatenate(y_label).flat)\n",
    "    #y_pred = list(np.concatenate(y_pred).flat)\n",
    "    #sorted_labels = sorted(labels,key=lambda name: (name[2:], name[0])) # group B and I results\n",
    "    print(classification_report(y_label, y_pred))#, labels=sorted_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceee52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    for idx in tqdm(range(len(training_data))):\n",
    "        postcheck_sent = prepare_sequence(training_data[idx][0], word_to_ix)\n",
    "        postcheck_sent = postcheck_sent.to(device)\n",
    "        pred =  model(postcheck_sent)[1]\n",
    "        y_pred.append([ix_to_tag[idx] for idx in pred])\n",
    "        y_label.append(training_data[idx][1])\n",
    "    print_classification_report(y_label, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    for idx in tqdm(range(len(validating_data))):\n",
    "        postcheck_sent = prepare_sequence(validating_data[idx][0], word_to_ix)\n",
    "        postcheck_sent = postcheck_sent.to(device)\n",
    "        pred =  model(postcheck_sent)[1]\n",
    "        y_pred.append([ix_to_tag[idx] for idx in pred])\n",
    "        y_label.append(validating_data[idx][1])\n",
    "    print_classification_report(y_label, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a245644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab09e4b1703455fb9499c46c780d4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "      Architecture       0.64      0.71      0.67       469\n",
      "           Command       0.07      0.05      0.06       245\n",
      "             Error       0.00      0.00      0.00       179\n",
      "         Extension       0.36      0.36      0.36        81\n",
      "                OS       0.52      0.61      0.56       448\n",
      "      Organization       0.13      0.22      0.16       360\n",
      "           Package       0.38      0.45      0.41       640\n",
      "        Peripheral       0.02      0.04      0.03       227\n",
      "Software_Component       0.00      0.00      0.00       163\n",
      "\n",
      "         micro avg       0.32      0.36      0.34      2812\n",
      "         macro avg       0.24      0.27      0.25      2812\n",
      "      weighted avg       0.31      0.36      0.33      2812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    ent = []\n",
    "    for idx in tqdm(range(len(testing_data))):\n",
    "        postcheck_sent = prepare_sequence(testing_data[idx][0], word_to_ix)\n",
    "        ent.append(testing_data[idx][0])\n",
    "        postcheck_sent = postcheck_sent.to(device)\n",
    "        pred =  model(postcheck_sent)[1]\n",
    "        y_pred.append([ix_to_tag[idx] for idx in pred])\n",
    "        y_label.append(testing_data[idx][1])\n",
    "    print_classification_report(y_label, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10452088",
   "metadata": {},
   "outputs": [],
   "source": [
    "fout_test = open('bilstm_pred.txt', 'w')\n",
    "\n",
    "for i in (range(len(y_pred))):\n",
    "    for j in (range(len(y_pred[i]))):\n",
    "        fout_test.write(str(ent[i][j].encode('utf-8')) + \"\\t\" + y_label[i][j] + \"\\t\" + y_pred[i][j] + \"\\n\")\n",
    "    fout_test.write(\"\\n\")\n",
    "fout_test.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ceb35f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad(), open('BiLSTMCRF_metrics.txt', 'a') as fout:\n",
    "#     y_pred = []\n",
    "#     y_label = []\n",
    "#     for idx in range(5000):\n",
    "#         if(idx % 500 == 0): \n",
    "#             print(idx)\n",
    "#         postcheck_sent = prepare_sequence(testing_data[idx][0], word_to_ix)\n",
    "#         postcheck_sent = postcheck_sent.to(device)\n",
    "#         pred =  model(postcheck_sent)[1]\n",
    "#         y_pred.append([ix_to_tag[idx] for idx in pred])\n",
    "#         y_label.append(testing_data[idx][1])\n",
    "#     print_classwise_f1_score(y_label, y_pred, fout)\n",
    "#     print_f1_score(y_label, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88baba98",
   "metadata": {},
   "source": [
    "#### Evaluation on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5964f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'G:/My Drive/Avik/NER_Baselines/BiLSTM_CRF_human_induced_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "749b095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('G:/My Drive/Avik/NER_Baselines/BiLSTM_CRF_human_induced_model.pth')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04a39a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183de0148fb0497e81380a053fe6e091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classwise accuracy : \n",
      "{'Command': 0.06, 'OS': 0.6538461538461539, 'Package': 0.450229709035222, 'Architecture': 0.6983122362869199, 'Organization': 0.34782608695652173, 'Peripheral': 0.12186978297161936, 'Error': 0.016881827209533268, 'Extension': 0.35802469135802467, 'Software_Component': 0.004807692307692308}\n",
      "The overall accuracy : 0.31449333871618895\n"
     ]
    }
   ],
   "source": [
    "## This code segment helps in finding out the accuracy of the model on the human data\n",
    "y_test = y_label\n",
    "classwise_acc_dict = {}\n",
    "classwise_count_dict = {}\n",
    "for i in tqdm(range(len(y_test))):\n",
    "    for j in range(len(y_test[i])):\n",
    "        if y_test[i][j] != 'O':\n",
    "            ent = y_test[i][j][2:]\n",
    "            \n",
    "            if ent not in classwise_count_dict:\n",
    "                classwise_count_dict[ent] = 0\n",
    "            classwise_count_dict[ent] = classwise_count_dict[ent] + 1\n",
    "            \n",
    "            if ent not in classwise_acc_dict:\n",
    "                classwise_acc_dict[ent] = 0\n",
    "            if y_pred[i][j][2:] == ent:\n",
    "                classwise_acc_dict[ent] = classwise_acc_dict[ent] + 1\n",
    "sum_correct = 0\n",
    "for key in classwise_acc_dict.keys():\n",
    "    sum_correct += classwise_acc_dict[key]\n",
    "    classwise_acc_dict[key] = classwise_acc_dict[key]/classwise_count_dict[key]\n",
    "print(\"The classwise accuracy : \")\n",
    "print(classwise_acc_dict)\n",
    "overall_accuracy = sum_correct/sum(classwise_count_dict.values())\n",
    "print(f\"The overall accuracy : {overall_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738f475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avik_2",
   "language": "python",
   "name": "avik_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
