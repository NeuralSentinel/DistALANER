{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3acdf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. change BERT model from multilingual to uncased\n",
    "# 2. analyse reason for 'nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://www.kaggle.com/code/realdeo/ner-crf-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82321073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm, trange\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8399f747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda_yes = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_yes else \"cpu\")\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a248539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b75999fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c4ea405",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for NER.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "          guid: Unique id for the example(a sentence or a pair of sentences).\n",
    "          words: list of words of sentence\n",
    "          labels_a/labels_b: (Optional) string. The label seqence of the text_a/text_b. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        # list of words of the sentence,example: [EU, rejects, German, call, to, boycott, British, lamb .]\n",
    "        self.words = words\n",
    "        # list of label sequence of the sentence,like: [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]\n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dfaeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\n",
    "    result of convert_examples_to_features(InputExample)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids,  predict_mask, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.predict_mask = predict_mask\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873ca3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CoNLLDataProcessor():\n",
    "    '''\n",
    "    CoNLL-2003\n",
    "    '''\n",
    "\n",
    "    def __init__(self, out_lists):\n",
    "        self.data = out_lists\n",
    "#         shuffle(self.data)\n",
    "        self._label_types = ['B-Command', 'B-Error', 'B-Extension', 'B-Software_Component', 'B-Peripheral',\n",
    "       'B-OS', 'B-Package', 'B-Architecture', 'B-Organization', 'I-Command', 'I-Error', 'I-Extension',\n",
    "       'I-Software_Component', 'I-Peripheral', 'I-OS', 'I-Package', 'I-Architecture', 'I-Organization', '[CLS]', '[SEP]','O']\n",
    "        self._num_labels = len(self._label_types)\n",
    "        self._label_map = {label: i for i,\n",
    "                           label in enumerate(self._label_types)}\n",
    "        self.train_data, self.test_data = train_test_split(self.data, test_size=0.2, random_state=1)\n",
    "        self.train_data, self.valid_data = train_test_split(self.train_data, test_size=0.25, random_state=1)\n",
    "\n",
    "    def get_train_examples(self):\n",
    "#         return self._create_examples(self.data[:len(self.data) * 3 // 5 ])\n",
    "        return self._create_examples(self.train_data)\n",
    "    \n",
    "    def get_valid_examples(self):\n",
    "#         return self._create_examples(self.data[len(self.data) * 3 // 5:len(self.data) * 4 // 5])\n",
    "        return self._create_examples(self.valid_data)\n",
    "    \n",
    "    def get_test_examples(self):\n",
    "#         return self._create_examples(self.data[len(self.data) * 4 // 5:])\n",
    "        return self._create_examples(self.test_data)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self._label_types\n",
    "\n",
    "    def get_num_labels(self):\n",
    "        return self._num_labels\n",
    "    \n",
    "    def get_label_map(self):\n",
    "        return self._label_map\n",
    "    \n",
    "    def get_start_label_id(self):\n",
    "        return self._label_map['[CLS]']\n",
    "\n",
    "    def get_stop_label_id(self):\n",
    "        return self._label_map['[SEP]']\n",
    "\n",
    "    def _create_examples(self, all_lists):\n",
    "        examples = []\n",
    "        for (i, one_lists) in enumerate(all_lists):\n",
    "            guid = i\n",
    "            words = one_lists[0]\n",
    "            labels = one_lists[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, words=words, labels=labels))\n",
    "        return examples\n",
    "\n",
    "    def _create_examples2(self, lines):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid = i\n",
    "            text = line[0]\n",
    "            ner_label = line[-1]\n",
    "            examples.append(InputExample(\n",
    "                guid=guid, text_a=text, labels_a=ner_label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aadf01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def example2feature(example, tokenizer, label_map, max_seq_length):\n",
    "\n",
    "    add_label = 'X'\n",
    "    # tokenize_count = []\n",
    "    tokens = ['[CLS]']\n",
    "    predict_mask = [0]\n",
    "    label_ids = [label_map['[CLS]']]\n",
    "    for i, w in enumerate(example.words):\n",
    "        # use bertTokenizer to split words\n",
    "        # 1996-08-22 => 1996 - 08 - 22\n",
    "        # sheepmeat => sheep ##me ##at\n",
    "        try:\n",
    "            if(pd.isna(w)):\n",
    "                sub_words = ['[UNK]']\n",
    "            else:\n",
    "                sub_words = tokenizer.tokenize(w)\n",
    "        except:\n",
    "            print(pd.isna(w))\n",
    "            print(type(w))\n",
    "            sub_words = tokenizer.tokenize(w)\n",
    "        if not sub_words:\n",
    "            sub_words = ['[UNK]']\n",
    "        # tokenize_count.append(len(sub_words))\n",
    "        tokens.extend(sub_words)\n",
    "        for j in range(len(sub_words)):\n",
    "            if j == 0:\n",
    "                predict_mask.append(1)\n",
    "                label_ids.append(label_map[example.labels[i]])\n",
    "            else:\n",
    "                # '##xxx' -> 'X' (see bert paper)\n",
    "                predict_mask.append(0)\n",
    "                label_ids.append(label_map[example.labels[i]])\n",
    "\n",
    "    # truncate\n",
    "    if len(tokens) > max_seq_length - 1:\n",
    "        #print('Example No.{} is too long, length is {}, truncated to {}!'.format(example.guid, len(tokens), max_seq_length))\n",
    "        tokens = tokens[0:(max_seq_length - 1)]\n",
    "        predict_mask = predict_mask[0:(max_seq_length - 1)]\n",
    "        label_ids = label_ids[0:(max_seq_length - 1)]\n",
    "    tokens.append('[SEP]')\n",
    "    predict_mask.append(0)\n",
    "    label_ids.append(label_map['[SEP]'])\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    segment_ids = [0] * len(input_ids)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    feat=InputFeatures(\n",
    "                # guid=example.guid,\n",
    "                # tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                predict_mask=predict_mask,\n",
    "                label_ids=label_ids)\n",
    "\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5328c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(data.Dataset):\n",
    "    def __init__(self, examples, tokenizer, label_map, max_seq_length):\n",
    "        self.examples=examples\n",
    "        self.tokenizer=tokenizer\n",
    "        self.label_map=label_map\n",
    "        self.max_seq_length=max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat=example2feature(self.examples[idx], self.tokenizer, \n",
    "                             self.label_map, self.max_seq_length)\n",
    "        return feat.input_ids, feat.input_mask, feat.segment_ids, feat.predict_mask, feat.label_ids\n",
    "\n",
    "    @classmethod\n",
    "    def pad(cls, batch):\n",
    "\n",
    "        seqlen_list = [len(sample[0]) for sample in batch]\n",
    "        maxlen = np.array(seqlen_list).max()\n",
    "\n",
    "        f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: X for padding\n",
    "        input_ids_list = torch.LongTensor(f(0, maxlen))\n",
    "        input_mask_list = torch.LongTensor(f(1, maxlen))\n",
    "        segment_ids_list = torch.LongTensor(f(2, maxlen))\n",
    "        predict_mask_list = torch.LongTensor(f(3, maxlen))\n",
    "        label_ids_list = torch.LongTensor(f(4, maxlen))\n",
    "\n",
    "        return input_ids_list, input_mask_list, segment_ids_list, predict_mask_list, label_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd5ebb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bug Id</th>\n",
       "      <th>Start Index</th>\n",
       "      <th>End Index</th>\n",
       "      <th>Entity</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>BIO Tag</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>143211</td>\n",
       "      <td>67</td>\n",
       "      <td>72</td>\n",
       "      <td>which</td>\n",
       "      <td>WDT</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>143211</td>\n",
       "      <td>73</td>\n",
       "      <td>75</td>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>143211</td>\n",
       "      <td>76</td>\n",
       "      <td>82</td>\n",
       "      <td>called</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>143211</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>143211</td>\n",
       "      <td>86</td>\n",
       "      <td>89</td>\n",
       "      <td>dav</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>143211</td>\n",
       "      <td>89</td>\n",
       "      <td>91</td>\n",
       "      <td>'s</td>\n",
       "      <td>POS</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>143211</td>\n",
       "      <td>92</td>\n",
       "      <td>99</td>\n",
       "      <td>PROPGET</td>\n",
       "      <td>NNP</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>143211</td>\n",
       "      <td>100</td>\n",
       "      <td>102</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>143211</td>\n",
       "      <td>103</td>\n",
       "      <td>111</td>\n",
       "      <td>retrieve</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>143211</td>\n",
       "      <td>112</td>\n",
       "      <td>115</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Bug Id  Start Index  End Index    Entity POS Tag BIO Tag  year\n",
       "10  143211           67         72     which     WDT       O  2004\n",
       "11  143211           73         75        is     VBZ       O  2004\n",
       "12  143211           76         82    called     VBN       O  2004\n",
       "13  143211           83         85        on      IN       O  2004\n",
       "14  143211           86         89       dav      NN       O  2004\n",
       "15  143211           89         91        's     POS       O  2004\n",
       "16  143211           92         99   PROPGET     NNP       O  2004\n",
       "17  143211          100        102        to      TO       O  2004\n",
       "18  143211          103        111  retrieve      VB       O  2004\n",
       "19  143211          112        115       the      DT       O  2004"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_df = pd.read_csv('~/sr_drive/Avik/Annotations/results/updated_merged_BIO_tagging_removing_verb_adjacent_pos_tags.csv')\n",
    "data_df.iloc[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96cf70fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_519910/1282516970.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  data_group = data_df.groupby(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bug Id</th>\n",
       "      <th>Entity</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>BIO Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>[Like, subscribing, to, a, bug, ,, I, 'd, like...</td>\n",
       "      <td>[IN, VBG, TO, DT, NN, ,, PRP, MD, VB, TO, VB, ...</td>\n",
       "      <td>[O, O, O, O, B-Package, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3165</td>\n",
       "      <td>[Symptoms, ========, Launchpad, sends, notific...</td>\n",
       "      <td>[NNS, VBP, NNP, VBZ, NNS, TO, NNS, IN, JJ, NNS...</td>\n",
       "      <td>[O, O, B-Organization, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3651</td>\n",
       "      <td>[I, 'm, looking, at, bzr, baz-import, ,, which...</td>\n",
       "      <td>[PRP, VBP, VBG, IN, JJ, NN, ,, WDT, VBZ, IN, P...</td>\n",
       "      <td>[O, O, O, O, B-Package, B-Command, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3666</td>\n",
       "      <td>['Track, Artist, ', is, a, great, concept, and...</td>\n",
       "      <td>[CD, NNP, '', VBZ, DT, JJ, NN, CC, PRP, VBP, P...</td>\n",
       "      <td>[O, B-Package, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3718</td>\n",
       "      <td>[While, installing, packages, ,, one, may, exp...</td>\n",
       "      <td>[IN, VBG, NNS, ,, CD, MD, VB, DT, NN, TO, VB, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-Package, O, O, O, B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bug Id                                             Entity  \\\n",
       "0     501  [Like, subscribing, to, a, bug, ,, I, 'd, like...   \n",
       "1    3165  [Symptoms, ========, Launchpad, sends, notific...   \n",
       "2    3651  [I, 'm, looking, at, bzr, baz-import, ,, which...   \n",
       "3    3666  ['Track, Artist, ', is, a, great, concept, and...   \n",
       "4    3718  [While, installing, packages, ,, one, may, exp...   \n",
       "\n",
       "                                             POS Tag  \\\n",
       "0  [IN, VBG, TO, DT, NN, ,, PRP, MD, VB, TO, VB, ...   \n",
       "1  [NNS, VBP, NNP, VBZ, NNS, TO, NNS, IN, JJ, NNS...   \n",
       "2  [PRP, VBP, VBG, IN, JJ, NN, ,, WDT, VBZ, IN, P...   \n",
       "3  [CD, NNP, '', VBZ, DT, JJ, NN, CC, PRP, VBP, P...   \n",
       "4  [IN, VBG, NNS, ,, CD, MD, VB, DT, NN, TO, VB, ...   \n",
       "\n",
       "                                             BIO Tag  \n",
       "0  [O, O, O, O, B-Package, O, O, O, O, O, O, O, O...  \n",
       "1  [O, O, B-Organization, O, O, O, O, O, O, O, O,...  \n",
       "2  [O, O, O, O, B-Package, B-Command, O, O, O, O,...  \n",
       "3  [O, B-Package, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "4  [O, O, O, O, O, O, O, O, B-Package, O, O, O, B...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_group = data_df.groupby(\n",
    "['Bug Id'],as_index=False\n",
    ")['Entity', 'POS Tag', 'BIO Tag'].agg(lambda x: list(x))\n",
    "\n",
    "data_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a90abb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If I insert in my CD-RW drive a CD labelled \"1\", then click on the diskmounter\n",
      "applet, the 2 choices I get will be \"Open CD-RW(null) Drive\" and \"Eject\n",
      "CD-RW(null) Drive\".  In Hoary, the choices would be the nicer \"Open 1\" and\n",
      "\"Eject 1\".\n",
      "\n",
      "http://bugzilla.gnome.org/show_bug.cgi?id=310300: http://bugzilla.gnome.org/show_bug.cgi?id=310300\n"
     ]
    }
   ],
   "source": [
    "# folder_path = '/content/drive/MyDrive/BTP/bugdataset/'\n",
    "folder_path = '~/sr_drive/Avik/Annotations/'\n",
    "bug_description_path = folder_path\n",
    "\n",
    "bug_description_df = pd.read_csv(bug_description_path + 'descriptions_merged.csv')\n",
    "\n",
    "for _, row in bug_description_df.iterrows():\n",
    "    if(row['Bug Id'] == 18886):\n",
    "        print(row['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ac60d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I installed the lkl 0.1.1-1 package from the universe repository. I'm running edgy on a Delll Inspron 6000. I setup lkl to run as a daemon normally started by init, but I can stop and start it with /etc/init.d/lkl stop/start.\n",
      "\n",
      "When tailing the outfile (/var/log/lkl/lkl.log) Everything starts out correctly, but almost immediately the time/date stamp disappears and an array of other characters begin to appear:\n",
      "\n",
      "root@paule2:~ # tail -F /var/log/lkl/lkl.log \n",
      "Wed Dec 13 12:28:18 2006\n",
      "wow<Ret>\n",
      "Wed Dec 13 12:28:28 2006\n",
      "sure<Ret>\n",
      "Wed Dec 13 12:28:36 2006\n",
      "<Ctrl>t<Tab><Ret>\n",
      "Wed Dec 13 12:29:14 2006\n",
      "<Tab>kk<Ret>\n",
      "Wed Dec 13 12:29:41 2006\n",
      "<Alt>NULL<Alt>NULLNULL���NULLNULLNULLNULL<Alt>NULL<Alt>NULL<Alt> MSD<Tab><Tab>DYNULL��NULLNULLEuroNULL�NULLNULLNULL���NULL ���NULLNULLNULL�EuroEuroNULL�NULL�<Shift>\"}}<Shift>{M \n"
     ]
    }
   ],
   "source": [
    "for _, row in bug_description_df.iterrows():\n",
    "    if(row['Bug Id'] == 75641):\n",
    "        print(row['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c3399e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "out_lists = []\n",
    "flag = 0\n",
    "for idx, row in data_group.iterrows():\n",
    "#     for w_idx, w in enumerate(row['Entity']):\n",
    "#         if(pd.isna(w)):\n",
    "#             print(w_idx)\n",
    "#             print(row['Entity'])\n",
    "#             print(row['Bug Id'])\n",
    "#             print(row['BIO Tag'][w_idx])\n",
    "#             flag += 1\n",
    "#             break\n",
    "#     if flag == 2:\n",
    "#         break\n",
    "    out_lists.append([row['Entity'], row['BIO Tag']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf7eb216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDistribution(examples):\n",
    "    labels = []\n",
    "    for example in examples:\n",
    "        labels.extend(example.labels)\n",
    "    label_frequency = {}\n",
    "\n",
    "    # iterating over the list\n",
    "    for label in labels:\n",
    "       # checking the element in dictionary\n",
    "        if label in label_frequency:\n",
    "          # incrementing the counr\n",
    "            label_frequency[label] += 1\n",
    "        else:\n",
    "          # initializing the count\n",
    "            label_frequency[label] = 1\n",
    "\n",
    "    # printing the frequency\n",
    "    print(label_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df976458",
   "metadata": {},
   "outputs": [],
   "source": [
    "conllProcessor = CoNLLDataProcessor(out_lists)\n",
    "label_list = conllProcessor.get_labels()\n",
    "label_map = conllProcessor.get_label_map()\n",
    "train_examples = conllProcessor.get_train_examples()\n",
    "valid_examples = conllProcessor.get_valid_examples()\n",
    "test_examples = conllProcessor.get_test_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e5b07fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 959\n",
      "{'O': 207233, 'B-OS': 2461, 'I-OS': 67, 'B-Command': 13304, 'B-Package': 12050, 'B-Peripheral': 803, 'B-Architecture': 794, 'B-Software_Component': 496, 'B-Extension': 505, 'I-Command': 583, 'B-Error': 92, 'I-Error': 257, 'B-Organization': 97, 'I-Package': 29}\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_examples)}')\n",
    "printDistribution(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1dab61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation examples: 320\n",
      "{'O': 70665, 'B-Command': 4549, 'B-Package': 4262, 'B-OS': 826, 'B-Architecture': 270, 'B-Peripheral': 262, 'B-Software_Component': 196, 'B-Extension': 360, 'I-Command': 222, 'B-Organization': 24, 'B-Error': 17, 'I-Error': 29, 'I-OS': 24, 'I-Package': 21, 'I-Peripheral': 2, 'I-Architecture': 1}\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of validation examples: {len(valid_examples)}')\n",
    "printDistribution(valid_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5906452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test examples: 320\n",
      "{'O': 75852, 'B-Package': 4477, 'B-OS': 816, 'B-Command': 4924, 'B-Architecture': 290, 'B-Peripheral': 310, 'B-Software_Component': 185, 'B-Error': 34, 'I-Error': 76, 'B-Organization': 30, 'B-Extension': 246, 'I-Command': 44, 'I-OS': 9, 'I-Package': 2, 'I-Peripheral': 1}\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of test examples: {len(test_examples)}')\n",
    "printDistribution(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d9fa422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7046f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list.remove('[CLS]')\n",
    "# label_list.remove('[SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aa72408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-Command',\n",
       " 'B-Error',\n",
       " 'B-Extension',\n",
       " 'B-Software_Component',\n",
       " 'B-Peripheral',\n",
       " 'B-OS',\n",
       " 'B-Package',\n",
       " 'B-Architecture',\n",
       " 'B-Organization',\n",
       " 'I-Command',\n",
       " 'I-Error',\n",
       " 'I-Extension',\n",
       " 'I-Software_Component',\n",
       " 'I-Peripheral',\n",
       " 'I-OS',\n",
       " 'I-Package',\n",
       " 'I-Architecture',\n",
       " 'I-Organization',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " 'O']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d784fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SpanBERT/spanbert-base-cased\")\n",
    "\n",
    "train_dataset = NerDataset(train_examples, tokenizer, label_map, 512)\n",
    "valid_dataset = NerDataset(valid_examples, tokenizer, label_map, 512)\n",
    "test_dataset = NerDataset(test_examples, tokenizer, label_map, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "899e6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4305b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=1,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "valid_dataloader = data.DataLoader(dataset=valid_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=1,\n",
    "                                collate_fn=NerDataset.pad)\n",
    "\n",
    "test_dataloader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=1,\n",
    "                                collate_fn=NerDataset.pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ac85af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "#Change to bert-base-uncased\n",
    "#bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"SpanBERT/spanbert-base-cased\")\n",
    "\n",
    "# bert_model.config.max_position_embeddings = 1024\n",
    "# bert_model.embeddings.position_ids = 1024\n",
    "# bert_model.embeddings.position_embeddings.weight.data = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dc3a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp_1vec(vec):  # shape(1,m)\n",
    "    max_score = vec[0, np.argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def log_sum_exp_mat(log_M, axis=-1):  # shape(n,m)\n",
    "    return torch.max(log_M, axis)[0]+torch.log(torch.exp(log_M-torch.max(log_M, axis)[0][:, None]).sum(axis))\n",
    "\n",
    "def log_sum_exp_batch(log_Tensor, axis=-1): # shape (batch_size,n,m)\n",
    "    return torch.max(log_Tensor, axis)[0]+torch.log(torch.exp(log_Tensor-torch.max(log_Tensor, axis)[0].view(log_Tensor.shape[0],-1,1)).sum(axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3081a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_CRF_NER(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_model, start_label_id, stop_label_id, num_labels, max_seq_length, batch_size, device):\n",
    "        super(BERT_CRF_NER, self).__init__()\n",
    "        self.hidden_size = 768\n",
    "        self.start_label_id = start_label_id\n",
    "        self.stop_label_id = stop_label_id\n",
    "        self.num_labels = num_labels\n",
    "        #self.max_seq_length = max_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.device=device\n",
    "\n",
    "        # use pretrainded BertModel \n",
    "        self.bert = bert_model\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        # Maps the output of the bert into label space.\n",
    "        self.hidden2label = nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.num_labels, self.num_labels))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer *to* the start tag(or label),\n",
    "        # and we never transfer *from* the stop label (the model would probably learn this anyway,\n",
    "        # so this enforcement is likely unimportant)\n",
    "        self.transitions.data[start_label_id, :] = -10000\n",
    "        self.transitions.data[:, stop_label_id] = -10000\n",
    "\n",
    "        nn.init.xavier_uniform_(self.hidden2label.weight)\n",
    "        nn.init.constant_(self.hidden2label.bias, 0.0)\n",
    "        # self.apply(self.init_bert_weights)\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)): \n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        '''\n",
    "        this also called alpha-recursion or forward recursion, to calculate log_prob of all barX \n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "        \n",
    "        # alpha_recursion,forward, alpha(zt)=p(zt,bar_x_1:t)\n",
    "        log_alpha = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        # normal_alpha_0 : alpha[0]=Ot[0]*self.PIs\n",
    "        # self.start_label has all of the score. it is log,0 is p=1\n",
    "        log_alpha[:, 0, self.start_label_id] = 0\n",
    "        \n",
    "        # feats: sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        # feats is the probability of emission, feat.shape=(1,tag_size)\n",
    "        for t in range(1, T):\n",
    "            log_alpha = (log_sum_exp_batch(self.transitions + log_alpha, axis=-1) + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # log_prob of all barX\n",
    "        log_prob_all_barX = log_sum_exp_batch(log_alpha)\n",
    "        return log_prob_all_barX\n",
    "\n",
    "    def _get_bert_features(self, input_ids, segment_ids, input_mask):\n",
    "        '''\n",
    "        sentances -> word embedding -> lstm -> MLP -> feats\n",
    "        '''\n",
    "        #bert_seq_out = self.bert(input_ids, token_type_ids=segment_ids, attention_mask=input_mask)[0]\n",
    "        bert_seq_out = self.bert(input_ids, attention_mask=input_mask)[0] # for spanBert\n",
    "        bert_seq_out = self.dropout(bert_seq_out)\n",
    "        bert_feats = self.hidden2label(bert_seq_out)\n",
    "        return bert_feats\n",
    "\n",
    "    def _score_sentence(self, feats, label_ids):\n",
    "        ''' \n",
    "        Gives the score of a provided label sequence\n",
    "        p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        batch_transitions = self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "        batch_transitions = batch_transitions.flatten(1)\n",
    "\n",
    "        score = torch.zeros((feats.shape[0],1)).to(device)\n",
    "        # the 0th node is start_label->start_word,\bthe probability of them=1. so t begin with 1.\n",
    "        for t in range(1, T):\n",
    "            score = score + \\\n",
    "                batch_transitions.gather(-1, (label_ids[:, t]*self.num_labels+label_ids[:, t-1]).view(-1,1)) \\\n",
    "                    + feats[:, t].gather(-1, label_ids[:, t].view(-1,1)).view(-1,1)\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        '''\n",
    "        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))\n",
    "        '''\n",
    "        \n",
    "        # T = self.max_seq_length\n",
    "        T = feats.shape[1]\n",
    "        batch_size = feats.shape[0]\n",
    "\n",
    "        # batch_transitions=self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n",
    "\n",
    "        log_delta = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n",
    "        log_delta[:, 0, self.start_label_id] = 0\n",
    "        \n",
    "        # psi is for the vaule of the last latent that make P(this_latent) maximum.\n",
    "        psi = torch.zeros((batch_size, T, self.num_labels), dtype=torch.long).to(self.device)  # psi[0]=0000 useless\n",
    "        for t in range(1, T):\n",
    "            # delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # delta[t] is the max prob of the path from  z_t-1 to z_t[k]\n",
    "            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n",
    "            # psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n",
    "            # psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1\n",
    "            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n",
    "\n",
    "        # trace back\n",
    "        path = torch.zeros((batch_size, T), dtype=torch.long).to(self.device)\n",
    "\n",
    "        # max p(z1:t,all_x|theta)\n",
    "        max_logLL_allz_allx, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n",
    "\n",
    "        for t in range(T-2, -1, -1):\n",
    "            # choose the state of z_t according the state choosed of z_t+1.\n",
    "            path[:, t] = psi[:, t+1].gather(-1,path[:, t+1].view(-1,1)).squeeze()\n",
    "\n",
    "        return max_logLL_allz_allx, path\n",
    "\n",
    "    def neg_log_likelihood(self, input_ids, segment_ids, input_mask, label_ids):\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "        forward_score = self._forward_alg(bert_feats)\n",
    "        # p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n",
    "        gold_score = self._score_sentence(bert_feats, label_ids)\n",
    "        # - log[ p(X=w1:t,Zt=tag1:t)/p(X=w1:t) ] = - log[ p(Zt=tag1:t|X=w1:t) ]\n",
    "        return torch.mean(forward_score - gold_score)\n",
    "\n",
    "    # this forward is just for predict, not for train\n",
    "    # dont confuse this with _forward_alg above.\n",
    "    def forward(self, input_ids, segment_ids, input_mask):\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, label_seq_ids = self._viterbi_decode(bert_feats)\n",
    "        return score, label_seq_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e015b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_label_id = conllProcessor.get_start_label_id()\n",
    "stop_label_id = conllProcessor.get_stop_label_id()\n",
    "\n",
    "model = BERT_CRF_NER(bert_model, start_label_id, stop_label_id, \n",
    "                     len(label_list), 512 , batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aad5a60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "valid_acc_prev = 0\n",
    "valid_f1_prev = 0\n",
    "\n",
    "model.to(device)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97227a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/somnath-am/anaconda3/envs/avik_2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "weight_decay_finetune = 1e-5 #0.01\n",
    "weight_decay_crf_fc = 5e-6 #0.005\n",
    "lr0_crf_fc = 8e-5\n",
    "learning_rate0 = 5e-5\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "new_param = ['transitions', 'hidden2label.weight', 'hidden2label.bias']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) \\\n",
    "        and not any(nd in n for nd in new_param)], 'weight_decay': weight_decay_finetune},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) \\\n",
    "        and not any(nd in n for nd in new_param)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in param_optimizer if n in ('transitions','hidden2label.weight')] \\\n",
    "        , 'lr':lr0_crf_fc, 'weight_decay': weight_decay_crf_fc},\n",
    "    {'params': [p for n, p in param_optimizer if n == 'hidden2label.bias'] \\\n",
    "        , 'lr':lr0_crf_fc, 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate0, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7603080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63df9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import classification_report\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3ef5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(report_dict):\n",
    "    df = pd.DataFrame(report_dict).transpose()\n",
    "    df.to_csv('BERT_CRF_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "639d36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, predict_dataloader, batch_size):\n",
    "    # print(\"***** Running prediction *****\")\n",
    "    global label_list\n",
    "    labels = label_list.copy()\n",
    "    labels.remove('[CLS]')\n",
    "    labels.remove('[SEP]')\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total=0\n",
    "    correct=0\n",
    "    start = time.time()\n",
    "    \n",
    "    inverted_map = {}\n",
    "    \n",
    "    for I in label_map:\n",
    "        inverted_map[label_map[I]] = I\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in predict_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "            score, label_seq_ids = model(input_ids, segment_ids, input_mask)\n",
    "            \n",
    "            predicted = np.array(label_seq_ids.cpu())\n",
    "            mask = np.array(predict_mask.cpu())\n",
    "            truth = np.array(label_ids.cpu())\n",
    "            \n",
    "            for M in range(len(mask)):\n",
    "                all_preds.append([inverted_map[I] for I in predicted[M][mask[M]]])\n",
    "                all_labels.append([inverted_map[I] for I in truth[M][mask[M]]])\n",
    "    #print(set(all_labels))\n",
    "    \n",
    "    report = classification_report(all_labels , all_preds)\n",
    "    #print(all_preds)\n",
    "    # Save report in a file\n",
    "    #report_dict = classification_report(all_labels , all_preds, output_dict=True, zero_division = 0)\n",
    "    #save_report(report_dict)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1c0c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b43e4c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_519910/1827738914.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for step, batch in  tqdm(enumerate(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d8ee9bd7764eb0975bd66fff7f5632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    240.    Elapsed: 136.80719828605652.\n",
      "  Batch   200  of    240.    Elapsed: 264.281676530838.\n",
      "--------------------------------------------------------------\n",
      "Epoch:0 completed, Total training's Loss: 1528196.6522216797, Spend: 5.191502026716868m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "classification_report() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--------------------------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m completed, Total training\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Spend: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, tr_loss, (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m train_start)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60.0\u001b[39m))\n\u001b[0;32m---> 43\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 34\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, predict_dataloader, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m             all_labels\u001b[38;5;241m.\u001b[39mappend([inverted_map[I] \u001b[38;5;28;01mfor\u001b[39;00m I \u001b[38;5;129;01min\u001b[39;00m truth[M][mask[M]]])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#print(set(all_labels))\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#print(all_preds)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Save report in a file\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#report_dict = classification_report(all_labels , all_preds, output_dict=True, zero_division = 0)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#save_report(report_dict)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n",
      "\u001b[0;31mTypeError\u001b[0m: classification_report() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "total_train_epochs = 5\n",
    "gradient_accumulation_steps = 1\n",
    "total_train_steps = int(len(train_examples) / batch_size / gradient_accumulation_steps * total_train_epochs)\n",
    "global_step_th = 0\n",
    "warmup_proportion = 0.1\n",
    "for epoch in range(total_train_epochs):\n",
    "        tr_loss = 0\n",
    "        train_start = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        for step, batch in  tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "            if step % 100 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = time.time() - train_start\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {}.'.format(step, len(train_dataloader), elapsed))\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n",
    "\n",
    "            neg_log_likelihood = model.neg_log_likelihood(input_ids, segment_ids, input_mask, label_ids)\n",
    "\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                neg_log_likelihood = neg_log_likelihood / gradient_accumulation_steps\n",
    "\n",
    "            neg_log_likelihood.backward()\n",
    "\n",
    "            tr_loss += neg_log_likelihood.item()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                # modify learning rate with special warm up BERT uses\n",
    "                lr_this_step = learning_rate0 * warmup_linear(global_step_th/total_train_steps, warmup_proportion)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_this_step\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step_th += 1\n",
    "\n",
    "        print('--------------------------------------------------------------')\n",
    "        print(\"Epoch:{} completed, Total training's Loss: {}, Spend: {}m\".format(epoch, tr_loss, (time.time() - train_start)/60.0))\n",
    "        evaluate(model, valid_dataloader, batch_size)\n",
    "# try:\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f112eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/somnath-am/anaconda3/envs/avik_2/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: [CLS] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/somnath-am/anaconda3/envs/avik_2/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CLS]       1.00      1.00      1.00     17592\n",
      "     Command       1.00      0.18      0.31       904\n",
      "   Extension       0.00      0.00      0.00       180\n",
      "          OS       1.00      0.30      0.47       841\n",
      "     Package       0.30      0.58      0.40      4726\n",
      "\n",
      "   micro avg       0.77      0.86      0.81     24243\n",
      "   macro avg       0.66      0.41      0.43     24243\n",
      "weighted avg       0.86      0.86      0.83     24243\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, test_dataloader, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'BERT_model/BERT_CRF.model'\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba78529",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_df = pd.read_csv('BERT_CRF_analysis.csv')\n",
    "crf_df.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avik_2",
   "language": "python",
   "name": "avik_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
