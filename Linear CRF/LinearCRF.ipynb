{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249e3d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use avik_2 environment with sklearn==0.23.2 version to run\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80fedb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from tqdm.auto import tqdm\n",
    "#import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "import random\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6acb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d90cea7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf7d3c022c64facb191e5a0b77fb365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88963a97ef8846f0ad4458f5876d0114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d802607a02f4448ba896b3dbfcd4c5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For Human Induced\n",
    "dataset_path = \"~/sr_drive/Avik/Annotations/results/finalized_dataset_human_induced/\"\n",
    "train_data = pd.read_csv(dataset_path + \"train.csv\")\n",
    "dev_data = pd.read_csv(dataset_path + \"dev.csv\")\n",
    "# test_data = pd.read_csv(dataset_path + \"test.csv\")\n",
    "test_data = pd.read_csv(\"~/sr_drive/Avik/Annotations/results/finalized_dataset/human_auto_test_dataset.csv\")\n",
    "\n",
    "train_sents = []\n",
    "valid_sents = []\n",
    "test_sents = []\n",
    "\n",
    "train_data = train_data.fillna(\"None\")\n",
    "dev_data = dev_data.fillna(\"None\")\n",
    "test_data = test_data.fillna(\"None\")\n",
    "\n",
    "for bug_id, group_df in tqdm(train_data.groupby('Bug Id')):\n",
    "    train_entry = []\n",
    "    for row_num, row in group_df.iterrows():\n",
    "        train_entry.append((row['Entity'], row['POS Tag'], row['BIO Tag']))\n",
    "    train_sents.append(train_entry)\n",
    "for bug_id, group_df in tqdm(dev_data.groupby('Bug Id')):\n",
    "    dev_entry = []\n",
    "    for row_num, row in group_df.iterrows():\n",
    "        dev_entry.append((row['Entity'], row['POS Tag'], row['BIO Tag']))\n",
    "    valid_sents.append(dev_entry)\n",
    "for bug_id, group_df in tqdm(test_data.groupby('Bug Id')):\n",
    "    test_entry = []\n",
    "    for row_num, row in group_df.iterrows():\n",
    "        test_entry.append((row['Entity'], row['POS Tag'], row['BIO Tag']))\n",
    "    test_sents.append(test_entry)\n",
    "    \n",
    "bio_tagging_labels = train_data['BIO Tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c72bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35ca522ef544f1d9e1b81ad89387381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cc643bd1e74f9ebbca8ffa2f600b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59aa280ed5ec470d925d97075070b18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for Launchpad\n",
    "dataset_path = \"~/sr_drive/Avik/Annotations/results/finalized_dataset_human_newsplit/\"\n",
    "train_data = pd.read_csv(dataset_path + \"train.csv\")\n",
    "dev_data = pd.read_csv(dataset_path + \"dev.csv\")\n",
    "test_data = pd.read_csv(\"/home/somnath-am/sr_drive/Avik/Launchpad/launchpad_bio_tagging_human.csv\")\n",
    "\n",
    "train_sents = []\n",
    "valid_sents = []\n",
    "test_sents = []\n",
    "\n",
    "train_data = train_data.fillna(\"None\")\n",
    "dev_data = dev_data.fillna(\"None\")\n",
    "test_data = test_data.fillna(\"None\")\n",
    "\n",
    "for bug_id, group_df in tqdm(train_data.groupby('Bug Id')):\n",
    "    train_entry = []\n",
    "    for row_num, row in group_df.iterrows():\n",
    "        train_entry.append((row['Entity'], row['POS Tag'], row['BIO Tag']))\n",
    "    train_sents.append(train_entry)\n",
    "for bug_id, group_df in tqdm(dev_data.groupby('Bug Id')):\n",
    "    dev_entry = []\n",
    "    for row_num, row in group_df.iterrows():\n",
    "        dev_entry.append((row['Entity'], row['POS Tag'], row['BIO Tag']))\n",
    "    valid_sents.append(dev_entry)\n",
    "for bug_id, group_df in tqdm(test_data.groupby('Bug Id')):\n",
    "    test_entry = []\n",
    "    for row_num, row in group_df.iterrows():\n",
    "        test_entry.append((row['Entity'], row['POS Tag'], row['BIO Tag']))\n",
    "    test_sents.append(test_entry)\n",
    "    \n",
    "bio_tagging_labels = train_data['BIO Tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"~/sr_drive/Avik/Annotations/results/finalized_dataset/\"\n",
    "bio_tagging_df = pd.read_csv(dataset_path + \"updated_merged_BIO_tagging_removing_verb_adjacent_pos_tags.csv\")\n",
    "test_data = pd.read_csv(dataset_path + \"human_test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd74ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"~/sr_drive/Avik/Launchpad/launchpad_bio_tagging.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb291212",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_tagging_labels = bio_tagging_df['BIO Tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_yrs = list(range(2004, 2014))\n",
    "valid_yrs = list(range(2014, 2016))\n",
    "test_yrs = list(range(2016, 2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb36473a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B-Package               696\n",
       "B-Software_Component    608\n",
       "B-OS                    595\n",
       "B-Peripheral            524\n",
       "B-Command               410\n",
       "I-Peripheral            347\n",
       "I-Error                 232\n",
       "B-Architecture          223\n",
       "I-OS                    208\n",
       "B-Organization          206\n",
       "I-Software_Component    125\n",
       "I-Organization          119\n",
       "I-Command               118\n",
       "B-Error                  88\n",
       "B-Extension              87\n",
       "I-Package                50\n",
       "I-Architecture            3\n",
       "Name: BIO Tag, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data support check\n",
    "test_data[(test_data['BIO Tag'] != 'O')]['BIO Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Do not run this\n",
    "# arr = test_data['Bug Id'].unique()\n",
    "# sampled_ids = np.random.choice(arr, size=round(len(arr)*0.4), replace=False)\n",
    "# bug_id_to_sents = {}\n",
    "# for idx in tqdm(range(bio_tagging_df.shape[0])):\n",
    "#     if bio_tagging_df['Bug Id'][idx] not in bug_id_to_sents:\n",
    "#         bug_id_to_sents[bio_tagging_df['Bug Id'][idx]] = []\n",
    "#     bug_id_to_sents[bio_tagging_df['Bug Id'][idx]].append((str(bio_tagging_df['Entity'][idx]), str(bio_tagging_df['POS Tag'][idx]), bio_tagging_df['BIO Tag'][idx], bio_tagging_df['year'][idx]))\n",
    "# train_sents = []\n",
    "# valid_sents = []\n",
    "# for bug_id, entry in tqdm(bug_id_to_sents.items()):\n",
    "#     if entry[0][-1] in train_yrs:\n",
    "#         train_sents.append([x[:-1] for x in entry])\n",
    "#     elif entry[0][-1] in valid_yrs:\n",
    "#         valid_sents.append([x[:-1] for x in entry])\n",
    "# test_sents = []\n",
    "# test_data = test_data.fillna(\"None\")\n",
    "# for bug_id, group_df in tqdm(test_data.groupby('Bug Id')):\n",
    "#     test_entry = []\n",
    "#     valid_entry = []\n",
    "#     if bug_id in sampled_ids:\n",
    "#         for row_num, row in group_df.iterrows():\n",
    "#             valid_entry.append((row['Entity'], row['POS Tag'], row['BIO Tag']))\n",
    "#         valid_sents.append(valid_entry)\n",
    "#     else:\n",
    "#         for row_num, row in group_df.iterrows():\n",
    "#             test_entry.append((row['Entity'], row['POS Tag'], row['BIO Tag']))\n",
    "#         test_sents.append(test_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "bug_id_to_sents = {}\n",
    "for idx in tqdm(range(bio_tagging_df.shape[0])):\n",
    "    if bio_tagging_df['Bug Id'][idx] not in bug_id_to_sents:\n",
    "        bug_id_to_sents[bio_tagging_df['Bug Id'][idx]] = []\n",
    "    bug_id_to_sents[bio_tagging_df['Bug Id'][idx]].append((str(bio_tagging_df['Entity'][idx]), str(bio_tagging_df['POS Tag'][idx]), bio_tagging_df['BIO Tag'][idx], bio_tagging_df['year'][idx]))\n",
    "train_sents = []\n",
    "valid_sents = []\n",
    "for bug_id, entry in tqdm(bug_id_to_sents.items()):\n",
    "    if entry[0][-1] in train_yrs:\n",
    "        train_sents.append([x[:-1] for x in entry])\n",
    "    elif entry[0][-1] in valid_yrs:\n",
    "        valid_sents.append([x[:-1] for x in entry])\n",
    "#     else:\n",
    "#         test_sents.append([x[:-1] for x in entry])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68def22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = []\n",
    "test_data = test_data.fillna(\"None\")\n",
    "for bug_id, group_df in tqdm(test_data.groupby('Bug Id')):\n",
    "    test_entry = []\n",
    "    for row_num, row in group_df.iterrows():\n",
    "        test_entry.append((row['Entity'], row['POS Tag'], row['BIO Tag']))\n",
    "    test_sents.append(test_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44d9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987187b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_sents, test_sents = train_test_split(sents, test_size=0.2, random_state=1)\n",
    "\n",
    "# train_sents, valid_sents = train_test_split(train_sents, test_size=0.25, random_state=1)\n",
    "# print(len(train_sents), len(valid_sents), len(test_sents), len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecfd99cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 s, sys: 4.19 s, total: 28.6 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_valid = [sent2features(s) for s in valid_sents]\n",
    "y_valid = [sent2labels(s) for s in valid_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f28db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def print_metrics(fout, y_label, y_pred):\n",
    "    y_label = list(np.concatenate(y_label).flat)\n",
    "    y_pred = list(np.concatenate(y_pred).flat)\n",
    "    fout.write(f'Accuracy score: {accuracy_score(y_label, y_pred)}\\n')\n",
    "\n",
    "def print_f1_score(fout, y_label, y_pred):\n",
    "    global bio_tagging_labels\n",
    "    y_label = list(np.concatenate(y_label).flat)\n",
    "    y_pred = list(np.concatenate(y_pred).flat)\n",
    "    labels = bio_tagging_labels\n",
    "#     labels=['B-Command', 'B-Error_description', 'B-Error_name', 'B-Extension',\n",
    "#        'B-OS', 'B-Package', 'B-URL', 'B-Ubuntu_distribution',\n",
    "#        'I-Error_description', 'I-Extension', 'I-URL', 'O']\n",
    "    classwise_f1_score = f1_score(y_label, y_pred, average=None, labels=labels)\n",
    "    fout.write('Classwise F-1 Score\\n')\n",
    "    for label,score in zip(labels, classwise_f1_score):\n",
    "        fout.write(str(label + ' : ' + str(score) + '\\n'))\n",
    "    fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5ce528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fout = open('CRF_logs.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76bf25f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'log_fout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:10\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_fout' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf1 = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=20,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf1.fit(X_train, y_train)\n",
    "print('Fit Done')\n",
    "log_fout.write(\"Training for algorithm 'lbfgs' done\\n\")\n",
    "\n",
    "with open('LinearCRF_metrics.txt', 'a') as fout:\n",
    "    y_valid_pred = crf1.predict(X_valid)\n",
    "    fout.write(f\"Algorithm: 'lbfgs'\\n\")\n",
    "    print_metrics(fout, y_valid, y_valid_pred)\n",
    "    print_f1_score(fout, y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3619703",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crf2 = sklearn_crfsuite.CRF(\n",
    "    algorithm='l2sgd',\n",
    "    c2=0.1,\n",
    "    max_iterations=20,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf2.fit(X_train, y_train)\n",
    "print('Fit Done')\n",
    "log_fout.write(\"Training for algorithm 'l2sgd' done\\n\")\n",
    "\n",
    "with open('LinearCRF_metrics.txt', 'a') as fout:\n",
    "    y_valid_pred = crf2.predict(X_valid)\n",
    "    fout.write(f\"Algorithm: 'l2sgd'\\n\")\n",
    "    print_metrics(fout, y_valid, y_valid_pred)\n",
    "    print_f1_score(fout, y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ff289",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crf3 = sklearn_crfsuite.CRF(\n",
    "    algorithm='ap',\n",
    "    max_iterations=20,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf3.fit(X_train, y_train)\n",
    "print('Fit Done')\n",
    "log_fout.write(\"Training for algorithm 'ap' done\\n\")\n",
    "\n",
    "with open('LinearCRF_metrics.txt', 'a') as fout:\n",
    "    y_valid_pred = crf3.predict(X_valid)\n",
    "    fout.write(f\"Algorithm: 'ap'\\n\")\n",
    "    print_metrics(fout, y_valid, y_valid_pred)\n",
    "    print_f1_score(fout, y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b35cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crf4 = sklearn_crfsuite.CRF(\n",
    "    algorithm='pa',\n",
    "    max_iterations=20,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf4.fit(X_train, y_train)\n",
    "print('Fit Done')\n",
    "log_fout.write(\"Training for algorithm 'pa' done\\n\")\n",
    "\n",
    "with open('LinearCRF_metrics.txt', 'a') as fout:\n",
    "    y_valid_pred = crf4.predict(X_valid)\n",
    "    fout.write(f\"Algorithm: 'pa'\\n\")\n",
    "    print_metrics(fout, y_valid, y_valid_pred)\n",
    "    print_f1_score(fout, y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cdb69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crf5 = sklearn_crfsuite.CRF(\n",
    "    algorithm='arow',\n",
    "    max_iterations=20,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf5.fit(X_train, y_train)\n",
    "print('Fit Done')\n",
    "log_fout.write(\"Training for algorithm 'arow' done\\n\")\n",
    "\n",
    "with open('LinearCRF_metrics.txt', 'a') as fout:\n",
    "    y_valid_pred = crf5.predict(X_valid)\n",
    "    fout.write(f\"Algorithm: 'arow'\\n\")\n",
    "    print_metrics(fout, y_valid, y_valid_pred)\n",
    "    print_f1_score(fout, y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557888b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crf6 = sklearn_crfsuite.CRF(\n",
    "    algorithm='pa',\n",
    "    max_iterations=40,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf6.fit(X_train, y_train)\n",
    "print('Fit Done')\n",
    "log_fout.write(\"Training for algorithm 'pa', # of iterations = 40\\n\")\n",
    "\n",
    "with open('LinearCRF_metrics.txt', 'a') as fout:\n",
    "    y_valid_pred = crf6.predict(X_valid)\n",
    "    fout.write(f\"Algorithm: 'pa', # of iterations = 40\\n\")\n",
    "    print_metrics(fout, y_valid, y_valid_pred)\n",
    "    print_f1_score(fout, y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a061b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crf7 = sklearn_crfsuite.CRF(\n",
    "    algorithm='pa',\n",
    "    max_iterations=60,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf7.fit(X_train, y_train)\n",
    "print('Fit Done')\n",
    "log_fout.write(\"Training for algorithm 'pa', # of iterations = 60\\n\")\n",
    "\n",
    "with open('LinearCRF_metrics.txt', 'a') as fout:\n",
    "    y_valid_pred = crf7.predict(X_valid)\n",
    "    fout.write(f\"Algorithm: 'pa', # of iterations = 60\\n\")\n",
    "    print_metrics(fout, y_valid, y_valid_pred)\n",
    "    print_f1_score(fout, y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5568e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "crf8 = sklearn_crfsuite.CRF(\n",
    "    algorithm='pa',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf8.fit(X_train, y_train)\n",
    "print('Fit Done')\n",
    "log_fout.write(\"Training for algorithm 'pa', # of iterations = 100\\n\")\n",
    "\n",
    "with open('LinearCRF_metrics.txt', 'a') as fout:\n",
    "    y_valid_pred = crf8.predict(X_valid)\n",
    "    fout.write(f\"Algorithm: 'pa', # of iterations = 100\\n\")\n",
    "    print_metrics(fout, y_valid, y_valid_pred)\n",
    "    print_f1_score(fout, y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d03e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Done\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "f1_score() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:14\u001b[0m\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mprint_f1_score\u001b[0;34m(fout, y_label, y_pred)\u001b[0m\n\u001b[1;32m     13\u001b[0m     labels \u001b[38;5;241m=\u001b[39m bio_tagging_labels\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     labels=['B-Command', 'B-Error_description', 'B-Error_name', 'B-Extension',\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#        'B-OS', 'B-Package', 'B-URL', 'B-Ubuntu_distribution',\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#        'I-Error_description', 'I-Extension', 'I-URL', 'O']\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     classwise_f1_score \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     fout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClasswise F-1 Score\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label,score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(labels, classwise_f1_score):\n",
      "\u001b[0;31mTypeError\u001b[0m: f1_score() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf9 = sklearn_crfsuite.CRF(\n",
    "    algorithm='pa',\n",
    "    max_iterations=150,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf9.fit(X_train, y_train)\n",
    "print('Fit Done')\n",
    "log_fout.write(\"Training for algorithm 'pa', # of iterations = 150\\n\")\n",
    "\n",
    "with open('LinearCRF_metrics.txt', 'a') as fout:\n",
    "    y_valid_pred = crf9.predict(X_valid)\n",
    "    fout.write(f\"Algorithm: 'pa', # of iterations = 150\\n\")\n",
    "    print_metrics(fout, y_valid, y_valid_pred)\n",
    "    print_f1_score(fout, y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceda7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LinearCRF_metrics.txt', 'a') as fout:\n",
    "    y_test_pred = crf9.predict(X_test)\n",
    "    fout.write(f\"Algorithm: 'pa', # of iterations = 150, on test set\\n\")\n",
    "    print_metrics(fout, y_test, y_test_pred)\n",
    "    print_f1_score(fout, y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7084094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import eli5\n",
    "# eli5.show_weights(crf, top=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e1f24ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-Package', 'B-Extension', 'B-Organization', 'B-Command', 'B-Software_Component', 'B-OS', 'B-Peripheral', 'I-Software_Component', 'B-Error', 'I-Package', 'B-Architecture', 'I-Organization', 'I-Error', 'I-Peripheral', 'I-Extension', 'I-OS', 'I-Architecture', 'I-Command']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/somnath-am/anaconda3/envs/avik_2/lib/python3.9/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=['B-Architecture', 'I-Architecture', 'B-Command', 'I-Command', 'B-Error', 'I-Error', 'B-Extension', 'I-Extension', 'B-OS', 'I-OS', 'B-Organization', 'I-Organization', 'B-Package', 'I-Package', 'B-Peripheral', 'I-Peripheral', 'B-Software_Component', 'I-Software_Component'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "      B-Architecture      0.962     0.951     0.956      1603\n",
      "      I-Architecture      0.800     0.320     0.457        25\n",
      "           B-Command      0.981     0.833     0.901      2189\n",
      "           I-Command      0.944     0.058     0.110       291\n",
      "             B-Error      0.950     0.722     0.820       756\n",
      "             I-Error      0.987     0.385     0.554      1582\n",
      "         B-Extension      0.973     0.966     0.970      1073\n",
      "         I-Extension      0.929     0.897     0.912        29\n",
      "                B-OS      0.910     0.916     0.913      2594\n",
      "                I-OS      0.744     0.480     0.584       641\n",
      "      B-Organization      0.905     0.825     0.863      1475\n",
      "      I-Organization      0.887     0.558     0.685       509\n",
      "           B-Package      0.928     0.892     0.909      7547\n",
      "           I-Package      0.893     0.118     0.209       211\n",
      "        B-Peripheral      0.949     0.848     0.896      1612\n",
      "        I-Peripheral      0.900     0.205     0.334       571\n",
      "B-Software_Component      0.967     0.888     0.926      2390\n",
      "I-Software_Component      0.951     0.395     0.558       147\n",
      "\n",
      "           micro avg      0.937     0.800     0.863     25245\n",
      "           macro avg      0.920     0.625     0.698     25245\n",
      "        weighted avg      0.937     0.800     0.844     25245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "y_pred = crf9.predict(X_test)\n",
    "labels = list(crf9.classes_)\n",
    "print(labels)\n",
    "labels.remove('O') # remove 'O' label from evaluation\n",
    "sorted_labels = sorted(labels,key=lambda name: (name[2:], name[0])) # group B and I results\n",
    "print(flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8163304e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "      Architecture       0.96      0.95      0.96      1603\n",
      "           Command       0.98      0.83      0.90      2189\n",
      "             Error       0.92      0.70      0.79       781\n",
      "         Extension       0.97      0.97      0.97      1073\n",
      "                OS       0.78      0.79      0.78      2594\n",
      "      Organization       0.82      0.75      0.78      1475\n",
      "           Package       0.93      0.89      0.91      7547\n",
      "        Peripheral       0.93      0.83      0.88      1612\n",
      "Software_Component       0.97      0.89      0.92      2392\n",
      "\n",
      "         micro avg       0.91      0.86      0.89     21266\n",
      "         macro avg       0.92      0.84      0.88     21266\n",
      "      weighted avg       0.92      0.86      0.89     21266\n",
      "\n",
      "Precision score : 0.9148541645785306\n",
      "Recall score : 0.858412489419731\n",
      "f1-score score : 0.8857350800582241\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "y_pred = crf9.predict(X_test)\n",
    "labels = list(crf9.classes_)\n",
    "labels.remove('O') # remove 'O' label from evaluation\n",
    "sorted_labels = sorted(labels,key=lambda name: (name[2:], name[0])) # group B and I results\n",
    "print(classification_report(y_test, y_pred))#, labels=sorted_labels, digits=3))\n",
    "print(\"Precision score : {}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score : {}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"f1-score score : {}\".format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "141fa813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f0fca274784849b1b56c63d57ece0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classwise accuracy : \n",
      "{'Command': 0.05333333333333334, 'OS': 0.6764705882352942, 'Package': 0.7366003062787136, 'Architecture': 0.959915611814346, 'Organization': 0.5384615384615384, 'Peripheral': 0.14691151919866444, 'Error': 0.1509433962264151, 'Extension': 0.6296296296296297, 'Software_Component': 0.1201923076923077}\n",
      "The overall accuracy : 0.44327815906338314\n"
     ]
    }
   ],
   "source": [
    "## This code segment helps in finding out the accuracy of the model on the human data\n",
    "classwise_acc_dict = {}\n",
    "classwise_count_dict = {}\n",
    "for i in tqdm(range(len(y_test))):\n",
    "    for j in range(len(y_test[i])):\n",
    "        if y_test[i][j] != 'O':\n",
    "            ent = y_test[i][j][2:]\n",
    "            \n",
    "            if ent not in classwise_count_dict:\n",
    "                classwise_count_dict[ent] = 0\n",
    "            classwise_count_dict[ent] = classwise_count_dict[ent] + 1\n",
    "            \n",
    "            if ent not in classwise_acc_dict:\n",
    "                classwise_acc_dict[ent] = 0\n",
    "            if y_pred[i][j][2:] == ent:\n",
    "                classwise_acc_dict[ent] = classwise_acc_dict[ent] + 1\n",
    "sum_correct = 0\n",
    "for key in classwise_acc_dict.keys():\n",
    "    sum_correct += classwise_acc_dict[key]\n",
    "    classwise_acc_dict[key] = classwise_acc_dict[key]/classwise_count_dict[key]\n",
    "print(\"The classwise accuracy : \")\n",
    "print(classwise_acc_dict)\n",
    "overall_accuracy = sum_correct/sum(classwise_count_dict.values())\n",
    "print(f\"The overall accuracy : {overall_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c15ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding the performance of model, to see if it is able to mark the entities marked by human\n",
    "def accuracy_of_model(y_test, y_pred):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b4d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_f1_score(y_label, y_pred):\n",
    "    y_label = list(np.concatenate(y_label).flat)\n",
    "    y_pred = list(np.concatenate(y_pred).flat)\n",
    "    print('Micro f1 score: ', f1_score(y_label, y_pred, average='micro'))\n",
    "    print('Macro f1 score: ', f1_score(y_label, y_pred, average='macro'))\n",
    "    print('Weighted f1 score: ', f1_score(y_label, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = crf1.predict(X_valid)\n",
    "print_f1_score(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = crf2.predict(X_valid)\n",
    "print_f1_score(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f999a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = crf3.predict(X_valid)\n",
    "print_f1_score(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68792389",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = crf4.predict(X_valid)\n",
    "print_f1_score(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = crf5.predict(X_valid)\n",
    "print_f1_score(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437fdb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = crf6.predict(X_valid)\n",
    "print_f1_score(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = crf7.predict(X_valid)\n",
    "print_f1_score(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68124e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = crf8.predict(X_valid)\n",
    "print_f1_score(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = crf9.predict(X_valid)\n",
    "print_f1_score(y_valid, y_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(crf9, \"Linear_CRF_human_induced.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf9 = torch.load(\"Linear_CRF_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6ec0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avik_2",
   "language": "python",
   "name": "avik_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
